{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Profner_mBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6e944243b0f425b93f038ad45ecf9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_658a8d86c9854ad6b3a02ca415e63245",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a69c27b5db934c1e8d03b291bd65c0a8",
              "IPY_MODEL_71b33d544800463abd0bcda9b978a446"
            ]
          }
        },
        "658a8d86c9854ad6b3a02ca415e63245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a69c27b5db934c1e8d03b291bd65c0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9476a5ec1abe4b9792f8f96841c09062",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 871891,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 871891,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0360eff5a27d49d3b40dc0d247255b2d"
          }
        },
        "71b33d544800463abd0bcda9b978a446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_75f362f9bb074aa299f1e424d1cb94ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872k/872k [00:00&lt;00:00, 1.91MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf8201a587574646bfd95d94e84d5004"
          }
        },
        "9476a5ec1abe4b9792f8f96841c09062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0360eff5a27d49d3b40dc0d247255b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75f362f9bb074aa299f1e424d1cb94ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf8201a587574646bfd95d94e84d5004": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "082141dba7964b48bc04ba93c222c306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8355d132b3b7469f8d4daa193cdc1bad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92737402d3784aa594be4d4b4a7e309c",
              "IPY_MODEL_429c45e6b38843d6b620809b557b29e4"
            ]
          }
        },
        "8355d132b3b7469f8d4daa193cdc1bad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92737402d3784aa594be4d4b4a7e309c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8642e8303a0b46d99aacf86b203989de",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 871891,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 871891,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6540808ba09f406186aea537167d5050"
          }
        },
        "429c45e6b38843d6b620809b557b29e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6322156768ed4512a8fc7bbb71a1c4f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872k/872k [00:00&lt;00:00, 6.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fda5a6e5ab054271a848c26a44948683"
          }
        },
        "8642e8303a0b46d99aacf86b203989de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6540808ba09f406186aea537167d5050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6322156768ed4512a8fc7bbb71a1c4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fda5a6e5ab054271a848c26a44948683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d809e439e3f4fd0ab69bb6da5df1ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b1cdca84f18d48f1a47aafdddc79017f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85fa47a0ce4a492d9a0540d7d7dd3d86",
              "IPY_MODEL_4f4c8083a0cb43f094a5e64e62aa08f0"
            ]
          }
        },
        "b1cdca84f18d48f1a47aafdddc79017f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85fa47a0ce4a492d9a0540d7d7dd3d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b07afb237b764ab29fe990c5d20fa4b6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 871891,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 871891,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4412aa196f254502b9ae36dff6058426"
          }
        },
        "4f4c8083a0cb43f094a5e64e62aa08f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f49a24adf75044f4bd13d1c4c50b5902",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872k/872k [00:00&lt;00:00, 6.08MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08f02ca8eb6e46559782fd96fd59eb30"
          }
        },
        "b07afb237b764ab29fe990c5d20fa4b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4412aa196f254502b9ae36dff6058426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f49a24adf75044f4bd13d1c4c50b5902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08f02ca8eb6e46559782fd96fd59eb30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2dbd60736734f91ba6980877c114799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4074726b0d2746b5b881a7306d38a560",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7056be218704b728e866b4a21c1f4b7",
              "IPY_MODEL_bf0bb9a6fa874f449e3eaea02dff5d37"
            ]
          }
        },
        "4074726b0d2746b5b881a7306d38a560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7056be218704b728e866b4a21c1f4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f10e923b924a48c2b8037b29ae45ab92",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_42913cd3e3a24a41b075e8e6d409a2af"
          }
        },
        "bf0bb9a6fa874f449e3eaea02dff5d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_62fc0b7264a445b2853a0b750e5d52dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:01&lt;00:00, 607B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38d417c033404de28ee1c9fe46c8b97a"
          }
        },
        "f10e923b924a48c2b8037b29ae45ab92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "42913cd3e3a24a41b075e8e6d409a2af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62fc0b7264a445b2853a0b750e5d52dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38d417c033404de28ee1c9fe46c8b97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0fc4f458ea74c1c85a42591f7707b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_727fb218ca7d41ee970db1a0baf9c684",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a78640614a9f4678ab0e6eeba73650fa",
              "IPY_MODEL_b7dbfbd1bfa946288718b07b952daf47"
            ]
          }
        },
        "727fb218ca7d41ee970db1a0baf9c684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a78640614a9f4678ab0e6eeba73650fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6c0dc70eee8648db8a5e442647e64e58",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 999358484,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 999358484,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0586ecabf744797b8543769de5aebec"
          }
        },
        "b7dbfbd1bfa946288718b07b952daf47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b19b78dda6c4b07b4db5ea1d5e1e9a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 999M/999M [00:30&lt;00:00, 33.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19c7314654144429a30efd3adf646c70"
          }
        },
        "6c0dc70eee8648db8a5e442647e64e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0586ecabf744797b8543769de5aebec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b19b78dda6c4b07b4db5ea1d5e1e9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19c7314654144429a30efd3adf646c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3e461ba429a4da29c3c3789a5e58b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_423892a869a54b40a23685a855388ad1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fc26d8cc0bc4a729f7c3504cd9edf7d",
              "IPY_MODEL_a304a9d8fb5a439fbbe69ef56be630e3"
            ]
          }
        },
        "423892a869a54b40a23685a855388ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fc26d8cc0bc4a729f7c3504cd9edf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0ddec9ab3ee44afa97e53baf993c3cc3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bab7d6da392644d59796bcb59e575315"
          }
        },
        "a304a9d8fb5a439fbbe69ef56be630e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_383557d2814d4d33a7e12121872928ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.16MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f636baaa3e0c4beb874da7289fd02ecb"
          }
        },
        "0ddec9ab3ee44afa97e53baf993c3cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bab7d6da392644d59796bcb59e575315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "383557d2814d4d33a7e12121872928ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f636baaa3e0c4beb874da7289fd02ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d283c9a299b5448da5bbcabbd99846b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9044a38dae054ae4a8a7eb380709d985",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2514ec8efbd347039dbf7b554bbc9353",
              "IPY_MODEL_7762d4e3fece4f519850e78860641088"
            ]
          }
        },
        "9044a38dae054ae4a8a7eb380709d985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2514ec8efbd347039dbf7b554bbc9353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_73e8bfbdffa640c897f252d4cac7516b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57029b5844f142e8aec00f4f8e2a37b2"
          }
        },
        "7762d4e3fece4f519850e78860641088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c244b2f535c74de3b6e58290c5af4eee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:08&lt;00:00, 48.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fb27c1e6cd145cc8828a471517c1077"
          }
        },
        "73e8bfbdffa640c897f252d4cac7516b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57029b5844f142e8aec00f4f8e2a37b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c244b2f535c74de3b6e58290c5af4eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fb27c1e6cd145cc8828a471517c1077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a74a122c5226443e9c7a59b48fadf326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f9da1799d88441ab61b3e4aff54922b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a44a9b5ce725468aa5500cfc610083f5",
              "IPY_MODEL_b95916ff8e8e48aca591e125a3919d12"
            ]
          }
        },
        "0f9da1799d88441ab61b3e4aff54922b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a44a9b5ce725468aa5500cfc610083f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f0d21e5443641bfbce6f2d641bebaf1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e8dad18d49145929ecb693a63c96d5c"
          }
        },
        "b95916ff8e8e48aca591e125a3919d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e78f25700eb048f4b2e8b3d6e2a995b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 536M/536M [00:08&lt;00:00, 61.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_100eeb72c0ce42eeaa7323bd3eeb410e"
          }
        },
        "3f0d21e5443641bfbce6f2d641bebaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e8dad18d49145929ecb693a63c96d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e78f25700eb048f4b2e8b3d6e2a995b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "100eeb72c0ce42eeaa7323bd3eeb410e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7b864fbb7e44ba288854c7358d94b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_993108f6cb9040b7bc2e193170fec178",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c49821795e44aeb8bc75e940b6f3536",
              "IPY_MODEL_3f3bf269c691472688a660e19837e46f"
            ]
          }
        },
        "993108f6cb9040b7bc2e193170fec178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c49821795e44aeb8bc75e940b6f3536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2af5eb70fa5e42b29568b580b2f40621",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_796422cd6ef84c8fae34b032eb21d7f7"
          }
        },
        "3f3bf269c691472688a660e19837e46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_725c403555944437b58eae4b1a22e511",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 2.12MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9f81e2b6bb74407914f92b834c50761"
          }
        },
        "2af5eb70fa5e42b29568b580b2f40621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "796422cd6ef84c8fae34b032eb21d7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "725c403555944437b58eae4b1a22e511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9f81e2b6bb74407914f92b834c50761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francesita/ProfnerTask7a/blob/main/Profner_mBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7L_oT3q2rdq"
      },
      "source": [
        "#Model for Profner Dataset Task 7\n",
        "\n",
        "Plan\n",
        "\n",
        "\n",
        "1.   Build baseline\n",
        "2.   Build model using BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2lk86045Xes",
        "outputId": "10f02294-f2a4-483c-e118-95d4356e596d"
      },
      "source": [
        "#mount googleDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH5rh8O62NIy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FylXGicA9xW"
      },
      "source": [
        "##Load datasets from csv files\n",
        "\n",
        "We load the data from csv files (it has already been separated, so we will not divide data between train, val, test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8lzbwVABThe"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/train.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reR_USik0WBi"
      },
      "source": [
        "#loading english data--- to augment positive examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROdasaMDCnIF",
        "outputId": "25a6b176-b5e7-467c-df7a-3abe09c7866d"
      },
      "source": [
        "print('amount of augmented data', len(aug_data_es))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amount of augmented data 865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdiq6unQDkTk"
      },
      "source": [
        "train = train_data.tweet\n",
        "aug =  aug_data_es.tweet\n",
        "val = val_data.tweet\n",
        "\n",
        "labels = train_data.label[:100]\n",
        "aug_labels = aug_data_es.label[:100]\n",
        "val_labels = val_data.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UnriF5bGJC9j",
        "outputId": "14a8f034-ef6a-43bd-b600-94a6fce157b1"
      },
      "source": [
        "type(aug[0])\n",
        "aug[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'denuncian a un internista que cuestionó si vale la pena salvar rojos del coronavirus'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB2nth1-COrd",
        "outputId": "656ea1e6-2b5e-4db4-dd2b-2fd98c2aac01"
      },
      "source": [
        "print(len(train), len(labels))\n",
        "print(len(aug), len(aug_labels))\n",
        "print(len(val), len(val_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 100\n",
            "100 100\n",
            "2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWylfzzi1eV"
      },
      "source": [
        "del train_data\n",
        "del val_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM2WMaAlEBsb"
      },
      "source": [
        "#concatenate traina nd augmented (for train) data\n",
        "train.append(aug)\n",
        "labels_aug = labels.append(aug_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFwIfGmVQkmG",
        "outputId": "3c64ff4a-9dc6-4a01-c911-e0676e6f56e8"
      },
      "source": [
        "print(train, \" \", labels)\n",
        "print(len(train_aug))\n",
        "print(len(labels_aug))\n",
        "type(labels[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     ['CHINA: libera una pandemia\\n', 'EE.UU: liber...\n",
            "1     ['San Francisco (EEUU) hace 100 años tras vivi...\n",
            "2     ['Porfi poneos la mascarilla o tendremos 28 nu...\n",
            "3     ['El nuevo „covid normas y reglas recibimiento...\n",
            "4     ['Si el confinamiento ha dejado algo tocada tu...\n",
            "                            ...                        \n",
            "95    ['@DGobiernoMadrid Gente que no sabe hasta cuá...\n",
            "96    ['Hoy a las 20h, concentración en tu centro de...\n",
            "97    ['Coronavirus | Bruselas da un toque a España ...\n",
            "98    ['Quienes no consideran necesaria la mascarill...\n",
            "99    ['Hoy damos las gracias a todo el personal san...\n",
            "Name: tweet, Length: 100, dtype: object   0     0\n",
            "1     0\n",
            "2     0\n",
            "3     0\n",
            "4     0\n",
            "     ..\n",
            "95    0\n",
            "96    0\n",
            "97    0\n",
            "98    0\n",
            "99    1\n",
            "Name: label, Length: 100, dtype: int64\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He0vYGIbEf5D"
      },
      "source": [
        "#Shuffle data\n",
        "idx = np.random.permutation(len(train))\n",
        "train, y_train = train[idx], labels[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfntiEYPCqRf"
      },
      "source": [
        "##Preprocess tweets \n",
        "\n",
        "1. remove twitter handles, links, hashtags, punctuation etc. so that we are left solely with text in the tweet\n",
        "2. tokenize tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl6mTbo_CKnh"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Accuracy, Recall\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pgFVGA4DOcg"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_tweets(tweet):\n",
        "  \"\"\"\n",
        "  - remove hashtags, twitter handles, url's\n",
        "  \"\"\"\n",
        "  #might want to try replacing @mentions with a word, such as person/persona?\n",
        "  #tweet = re.findall(r'#(\\w+)', tweet, re.UNICODE)\n",
        "  tweet = ' '.join(re.sub(r\"(@[A-Za-z0-9_]+)|([áéíóúñü][^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet).split())\n",
        "  tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \")\n",
        "  #remove emojis from tweets\n",
        "  #tweet = remove_emoji(tweet)\n",
        "  return tweet"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K49D2Kb541_"
      },
      "source": [
        "import string\n",
        "def remove_punctuation(tknzd_tweet):\n",
        "  spanish_pnct = (['¿','¡',':','¨','...',\"'\",'€','£','$','\"','@'])\n",
        "  cln_twt = []\n",
        "  for token in tknzd_tweet:\n",
        "    if token not in string.punctuation and token not in spanish_pnct:\n",
        "      cln_twt.append(token)\n",
        "  \n",
        "  return cln_twt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXgZ2qEJ-uF7",
        "outputId": "7badade6-f601-44b3-b2b0-0b53894ed7ee"
      },
      "source": [
        "!pip install demoji\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "#remove emojis from tweet\n",
        "def remove_emoji(tweet):\n",
        "  tweet = demoji.replace(tweet, \"\")\n",
        "\n",
        "  return tweet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.4 demoji-0.4.0\n",
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.16 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P-BZv3rdq1B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4389741d-bbf9-4a25-b8c8-a3829d4fa82d"
      },
      "source": [
        "text = 'Cuñado, vén conmigo mañana para comer arroz. ¡Tú eres tán estúpida! @lal_oca #ror_dfa 😜🇵🇷💀⛵🈵🇸🇻 @'\n",
        "preprocess_tweets(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Cuñado, vén conmigo mañana para comer arroz. ¡Tú eres tán estúpida! ror dfa  @'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byyn86ytDxYs"
      },
      "source": [
        "We can check that our data is properly encoded doing the following"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmm2memefp7L"
      },
      "source": [
        "#Model using mBert Sequence Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPRsqx0zyzbL"
      },
      "source": [
        "Load data for the model. The data needs to be encoded differently from the BiLSTM classifier above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRR4nRiFyC4m"
      },
      "source": [
        "#import data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/train.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "365Hlr2T0t5P"
      },
      "source": [
        "\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/test.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjO62GcwiaLd"
      },
      "source": [
        "test = test_data.tweet"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja2xWGWFTPRV"
      },
      "source": [
        "es_train = list(train_data.tweet)\n",
        "#eng_train = list(eng_data.tweet)\n",
        "val = val_data.tweet\n",
        "\n",
        "es_labels = list(train_data.label)\n",
        "#eng_labels = list(eng_data.label) \n",
        "val_labels = val_data.label"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fB7tMji1NOP"
      },
      "source": [
        "#turn train data lists to numpy arrays\n",
        "train = np.asarray(es_train)\n",
        "#eng_train = np.asarray(eng_train)\n",
        "\n",
        "labels = np.asarray(es_labels)\n",
        "#eng_labels= np.asarray(eng_labels)\n",
        "\n",
        "#Combine training data\n",
        "#train = np.append(es_train, eng_train)\n",
        "#labels = np.append(es_labels, eng_labels)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC3Srg4gRRpk",
        "outputId": "3e6af823-09ba-424f-8c22-5b63cb8e5e5d"
      },
      "source": [
        "len(train)\n",
        "print(labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIdsjnaE07wj"
      },
      "source": [
        "#shuffle data for both train and val sets\n",
        "idx = np.random.permutation(len(train))\n",
        "x_train, y_train = train[idx],labels[idx]\n",
        "\n",
        "x_val, y_val = val,val_labels"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04IW9EXYNxv9",
        "outputId": "f3b3ef30-560e-4af3-e2a2-b6e7d35b1f5b"
      },
      "source": [
        "print(len(x_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBKWjahs1tM2"
      },
      "source": [
        "##Text processing for mBERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq9AH_hR1s4o",
        "outputId": "5dd1bed7-c62e-44a8-951a-fe2d52198bcc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "     |████████████████████████████████| 1.9MB 17.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "     |████████████████████████████████| 890kB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "     |████████████████████████████████| 3.2MB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=4a02a2d87e297da9a63e8acceb632a5232a811f4941547d97b4a08c3799a5d5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgMOgtqu2_b7"
      },
      "source": [
        "##Preprocess tweet for mBERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyJBr70W8Qcb",
        "outputId": "a86fec37-1a84-4550-8c7d-56b64c6d42b5"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "847cQR4ZpUUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "d6e944243b0f425b93f038ad45ecf9e6",
            "658a8d86c9854ad6b3a02ca415e63245",
            "a69c27b5db934c1e8d03b291bd65c0a8",
            "71b33d544800463abd0bcda9b978a446",
            "9476a5ec1abe4b9792f8f96841c09062",
            "0360eff5a27d49d3b40dc0d247255b2d",
            "75f362f9bb074aa299f1e424d1cb94ed",
            "bf8201a587574646bfd95d94e84d5004"
          ]
        },
        "outputId": "43962573-75b9-415e-a5d0-a0e08c32e957"
      },
      "source": [
        "#load mBERT tokenizer, we will use it later in a function (this one is used to encode to find max_len)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6e944243b0f425b93f038ad45ecf9e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVIxrKDhpWEX",
        "outputId": "d37fcbeb-5e38-4357-9dc5-bb9c11716b96"
      },
      "source": [
        "tokenizer.encode(\"Ayer me cague en la puta madre de este trabajo\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 11612,\n",
              " 10177,\n",
              " 10525,\n",
              " 10678,\n",
              " 17734,\n",
              " 10109,\n",
              " 10106,\n",
              " 28721,\n",
              " 15452,\n",
              " 10102,\n",
              " 10494,\n",
              " 15858,\n",
              " 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jlj3XDS9GO0"
      },
      "source": [
        "#defining function to preprocess with Bert\n",
        "def preprocess_bert(tweets, pt_tokenizer_model, max_len):\n",
        "    '''\n",
        "    This function will do a variety of things to prep data for Roberta (through encode_plus) which includes:\n",
        "      -tokenizing tweet\n",
        "      -adding <s> BOS token (used for classification) and </s></s> as a [SEP] token to start and end of tweet\n",
        "      -pad or truncate the tweet to max length\n",
        "      -map tokens to their encoding or id\n",
        "      -creates attention mask: this is a mask used for attention when a batch has varying length of sentences\n",
        "      -returns a dict of outputs\n",
        "      -all this comes from fine-tuning tutorial from skimai.com\n",
        "    '''\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(pt_tokenizer_model, do_lower_case = True)\n",
        "\n",
        "    encoded_tweets = []\n",
        "    #attention masks indicated to the model we will use, which tokens should be attended to, example indicates position of padding\n",
        "    #so model should not pay attention to these. \n",
        "    attention_masks = []\n",
        "\n",
        "\n",
        "    for tweet in tweets:\n",
        "        encoded_tweet = tokenizer.encode_plus(\n",
        "            text = preprocess_tweets(tweet),\n",
        "            add_special_tokens=True, #cls and sep tokens \n",
        "            max_length=max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True)\n",
        "\n",
        "        #add outputs to list\n",
        "        encoded_tweets.append(encoded_tweet.get('input_ids'))\n",
        "        attention_masks.append(encoded_tweet.get('attention_mask'))\n",
        "      \n",
        "    #we now convert the lists to tensors\n",
        "    encoded_tweets = np.asarray(encoded_tweets, dtype='int32')\n",
        "    attention_masks = np.asarray(attention_masks, dtype='int32')\n",
        "\n",
        "    return encoded_tweets, attention_masks\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMGi08AeBWAo"
      },
      "source": [
        "#Now we find maximum length in our list of tweets with the special tokens\n",
        "\n",
        "#we concatenate training and validation sets \n",
        "all_tweets = np.concatenate([train_data.tweet, val_data.tweet])\n",
        "\n",
        "#encode using tokenizer function (not our fucntion with encoded_plus, this is because we want to see max_length of tweet with special tokens)\n",
        "all_tweets_encoded = [tokenizer.encode(tweet, add_special_tokens=True) for tweet in all_tweets]\n",
        "print(all_tweets_encoded[0])\n",
        "print(tokenizer.decode(all_tweets_encoded[0]))\n",
        "\n",
        "#find maximum length\n",
        "max_len = max([len(tweet) for tweet in all_tweets_encoded])\n",
        "print('max length of tweets:', max_len)\n",
        "\n",
        "#find avg length\n",
        "# we might want to use avg len rather than max length in future\n",
        "avg_len = int(np.average([len(tweet) for tweet in all_tweets_encoded]))\n",
        "print('average len:' ,avg_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us163M31fIJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbaee7d-6a43-491a-ebc9-81802a9b29b1"
      },
      "source": [
        "print(type(x_train))\n",
        "print(x_train[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "['🔝#SoniaPérez: \"Podemos conseguir que el #COVID19 no corra tanto quedándonos en casa, pero para que eso sea posible algunos tienen que salir de la suya\"\\n', '\\n', '🗞️Artículo en @elcorreo_com  de la consejera socialista de Turismo, Comercio y Consumo del Gobierno Vasco \\n', '\\n', '#FuertesYUnidos https://t.co/yuCdjWTjnI']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "082141dba7964b48bc04ba93c222c306",
            "8355d132b3b7469f8d4daa193cdc1bad",
            "92737402d3784aa594be4d4b4a7e309c",
            "429c45e6b38843d6b620809b557b29e4",
            "8642e8303a0b46d99aacf86b203989de",
            "6540808ba09f406186aea537167d5050",
            "6322156768ed4512a8fc7bbb71a1c4f7",
            "fda5a6e5ab054271a848c26a44948683"
          ]
        },
        "id": "xHM3_wjCbf4O",
        "outputId": "efa39451-8933-4e61-c74e-e458b6844c6f"
      },
      "source": [
        "#define var with model type\n",
        "m_tokenizer = 'bert-base-multilingual-uncased'\n",
        "max_len = 80\n",
        "#tokenize our tweets using the pre-proprocess function we defined earlier\n",
        "train_inputs, train_masks = preprocess_bert(x_train, m_tokenizer, max_len)\n",
        "val_inputs, val_masks = preprocess_bert(x_val, m_tokenizer, max_len)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "082141dba7964b48bc04ba93c222c306",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNGaDZKDiuyl",
        "outputId": "a14f14ed-4f74-4909-aae7-1246999c844b"
      },
      "source": [
        "#tokenize and preprocess test\n",
        "m_tokenizer = 'bert-base-multilingual-uncased'\n",
        "test_inputs, test_masks = preprocess_bert(test, m_tokenizer, max_len)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl8GXzLr9Lye",
        "outputId": "f0bcb4ae-7f8e-47bb-ba12-f5ab12945f43"
      },
      "source": [
        "print(train_inputs[0])\n",
        "print(train_masks[0])\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  101   138   112 10265 16422 10107 10128 23145 41881 11823 10119 12715\n",
            "   110 10109 10117 29346 10102 10106 84315 10102 20241 70294   117 10173\n",
            " 11589   119 10292 46720 38183 10321 10109 10426 17530 11310 10190   112\n",
            "   140   102     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpcqUA4y-KEb",
        "outputId": "0f81c261-f0ce-4da6-8cea-9699488ae7d6"
      },
      "source": [
        "print('shape of test_inputs:', test_inputs.shape)\n",
        "print('shape of test_masks', test_masks.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of test_inputs: (27000, 80)\n",
            "shape of test_masks (27000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XikBiAni8qX"
      },
      "source": [
        "print('shape of train_inputs:', train_inputs.shape)\n",
        "print('shape of train_masks', train_masks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxC9Sgy1zrMH"
      },
      "source": [
        "##Fine-tune mBERT classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALYx2cOh-dUD"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#making labels into numpy arrays for training in transformers and keras\n",
        "def create_transformer_model(pre_trained_model, num_labels, max_len):\n",
        "  transformer_model = TFBertForSequenceClassification.from_pretrained(pre_trained_model, num_labels=num_labels)\n",
        "\n",
        "  #We create keras tensors\n",
        "  input_ids = tf.keras.layers.Input(shape=(max_len,), name='train_input', dtype=tf.int32)\n",
        "  input_masks = tf.keras.layers.Input(shape=(max_len), name='train_masks', dtype=tf.int32)\n",
        "\n",
        "  #take into account single dimension\n",
        "  seq_outputs = transformer_model(input_ids, input_masks)[0]\n",
        "  outputs = tf.keras.layers.Dense(num_labels, activation='sigmoid')(seq_outputs)\n",
        "\n",
        "  model = tf.keras.models.Model(inputs=[input_ids,input_masks], outputs=outputs)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klMngltfWuI",
        "outputId": "f22ad500-fc2b-4f34-b7c2-3eb401664901"
      },
      "source": [
        "pt_trans_model = 'bert-base-multilingual-uncased'\n",
        "num_labels = 1\n",
        "loss = 'binary_crossentropy'\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "\n",
        "model = create_transformer_model(pt_trans_model, num_labels, 80)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 167357185   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 167,357,187\n",
            "Trainable params: 167,357,187\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ6j0jXZ1zTS"
      },
      "source": [
        "#make labels into numpy arrays, and prep x_val\n",
        "x_val = [val_inputs, val_masks]\n",
        "#turn labels into tensors\n",
        "y_train = np.asarray(y_train, dtype='int32')\n",
        "y_val = np.asarray(y_val, dtype='int32')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2vT53F60a9A",
        "outputId": "7b2628a0-d492-44db-e294-4f0d206f8638"
      },
      "source": [
        "!nvidia-smi -L\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-927f2c9a-a68c-2ff6-4af2-ada1a4bda9d7)\n",
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ULINSSjLw2",
        "outputId": "0a524857-fd80-4060-feef-f3e06fc41051"
      },
      "source": [
        "#fit model to input data\n",
        "#class_weight = {0: 1.,\n",
        "#                1: 3.}\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model.fit([train_inputs, train_masks], y_train, validation_data=(x_val, y_val), epochs=3, batch_size = 32)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7493WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "188/188 [==============================] - 120s 588ms/step - loss: 0.5526 - accuracy: 0.7494 - val_loss: 0.2940 - val_accuracy: 0.9035\n",
            "Epoch 2/3\n",
            "188/188 [==============================] - 112s 595ms/step - loss: 0.2637 - accuracy: 0.9073 - val_loss: 0.1859 - val_accuracy: 0.9420\n",
            "Epoch 3/3\n",
            "188/188 [==============================] - 112s 594ms/step - loss: 0.1448 - accuracy: 0.9575 - val_loss: 0.1675 - val_accuracy: 0.9480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAdUtrvy06su"
      },
      "source": [
        "#save model\n",
        "model.save_weights('/content/drive/MyDrive/Colab/saved_model/mBert_profner_emoji.hdf5')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FKYnynBEaSc"
      },
      "source": [
        "##Testing and Error Analysis on mBert Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flOryVfl1bjb"
      },
      "source": [
        "#load model if not already\n",
        "model.load_weights('/content/drive/MyDrive/Colab/saved_model/mBert_profner3.hdf5')\n",
        "#del es_model\n",
        "#es_model = tf.saved_model.load('/content/drive/MyDrive/Colab/saved_model/mBert_profner3')\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6zGa5XOG0uL"
      },
      "source": [
        "def assign_class(a_pred):\n",
        "  pred = []\n",
        "  for i in range(len(a_pred)):\n",
        "    if a_pred[i] >= 0.5:\n",
        "      pred.append(1)\n",
        "    else:\n",
        "      pred.append(0)\n",
        "  return pred"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDZwrJhDExqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7e9c2e-6d5f-42dc-f1bd-2fe27ee9beed"
      },
      "source": [
        "#make predictions on model\n",
        "y_pred = model.predict([val_inputs, val_masks])\n",
        "es_pred = assign_class(y_pred)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9fG-wHUo0T9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a9ca48-a353-4a55-d02e-49a12005cc97"
      },
      "source": [
        "print(y_pred[:10])\n",
        "print(es_pred[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00605112]\n",
            " [0.00700646]\n",
            " [0.9257546 ]\n",
            " [0.043198  ]\n",
            " [0.00858568]\n",
            " [0.01304464]\n",
            " [0.00830286]\n",
            " [0.01447904]\n",
            " [0.9908717 ]\n",
            " [0.00924681]]\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2BgBL2bICID",
        "outputId": "fc91f7b2-e0a7-4270-e9da-83c96e6d585b"
      },
      "source": [
        "#see what is going wrong\n",
        "print(es_pred[:20])\n",
        "print(type(val))\n",
        "es_pred_dic = {} #index of tweet is key and values are predicted value\n",
        "for i, p in enumerate(es_pred):\n",
        "  if y_val[i] != p:\n",
        "    es_pred_dic[i] = {}\n",
        "    es_pred_dic[i]['tweet'] = val[i]\n",
        "    es_pred_dic[i]['pred'] = p\n",
        "    es_pred_dic[i]['true'] = y_val[i]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n",
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFeMOWFB0GbZ",
        "outputId": "0b372186-cf6e-4d91-aff1-503ed472c5cf"
      },
      "source": [
        "len(es_pred_dic.items())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4WrEpNFATR"
      },
      "source": [
        "###Confusion Matrix for mBert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNGvErUYFcQ7",
        "outputId": "42a54b43-a7ca-4079-f918-0fb6113616bf"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = y_val\n",
        "y_pred = es_pred\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred, labels=[1,0]))\n",
        "print(classification_report(y_true, y_pred, target_names=[\"non-prof\", \"prof\"]))\n",
        "print('f1-score:', f1_score(y_true, y_pred))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 401   76]\n",
            " [  28 1495]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    non-prof       0.95      0.98      0.97      1523\n",
            "        prof       0.93      0.84      0.89       477\n",
            "\n",
            "    accuracy                           0.95      2000\n",
            "   macro avg       0.94      0.91      0.93      2000\n",
            "weighted avg       0.95      0.95      0.95      2000\n",
            "\n",
            "f1-score: 0.8852097130242826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEeLN8Ljyi9f"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6z14DMDjvFF"
      },
      "source": [
        "#Model using Bert Uncased Classifier\n",
        "We train a model using bert uncased using the English translation of the tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u72gyhnl9Z7"
      },
      "source": [
        "#load english tweet data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/train-eng.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val-eng.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbadP7eCIH63"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTJRao02mFks"
      },
      "source": [
        "eng_train = train_data.tweet\n",
        "eng_val = val_data.tweet\n",
        "\n",
        "labels = train_data.label\n",
        "val_labels = val_data.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxZsbYTMmSJz"
      },
      "source": [
        "#shuffle data for both train and val sets\n",
        "idx = np.random.permutation(len(eng_train))\n",
        "x_train, y_train = eng_train[idx],labels[idx]\n",
        "\n",
        "x_val, y_val = eng_val, val_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYdhImHS7iuw",
        "outputId": "8813747d-740b-4d80-c8a1-08b82fbe9716"
      },
      "source": [
        "print(x_val[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  101   138   112 ...     0     0     0]\n",
            " [  101   138   112 ...     0     0     0]\n",
            " [  101   138   112 ...     0     0     0]\n",
            " ...\n",
            " [  101   138   112 ...   136 76320   102]\n",
            " [  101   138   112 ...     0     0     0]\n",
            " [  101   138   112 ...     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8eCMBIbkvM9"
      },
      "source": [
        "##Preprocess tweet for Bert\n",
        "- we use a function preprocess_bert defined earlier in the code the encode our tweets, except we encode tweets to the pretrained bert-uncased model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSM0Q4qFljnK",
        "outputId": "f1081bca-ecb1-4f15-8864-68e15530266c"
      },
      "source": [
        "#define var with model type\n",
        "eng_tokenizer = 'bert-base-uncased'\n",
        "max_len = 80\n",
        "#preprocess using bert\n",
        "eng_train_input, eng_train_masks = preprocess_bert(x_train, eng_tokenizer, max_len) \n",
        "eng_val_input, eng_val_masks = preprocess_bert(x_val, eng_tokenizer, max_len) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1urVHOoPwJ"
      },
      "source": [
        "##Fine-Tune Bert uncased classification model\n",
        "We use the create_model function defined in the mBert section. We will provide the transformer model type (bert-uncased in our case), the number of classes in our problem, and max_len of each tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZMyABof8K0y"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#making labels into numpy arrays for training in transformers and keras\n",
        "def create_eng_transformer_model(pre_trained_model, num_labels, max_len):\n",
        "  transformer_model = TFBertForSequenceClassification.from_pretrained(pre_trained_model, num_labels=num_labels)\n",
        "\n",
        "  #We create keras tensors\n",
        "  input_ids = tf.keras.layers.Input(shape=(max_len,), name='train_input', dtype=tf.int32)\n",
        "  input_masks = tf.keras.layers.Input(shape=(max_len), name='train_masks', dtype=tf.int32)\n",
        "\n",
        "  #take into account single dimension\n",
        "  seq_outputs = transformer_model(input_ids, input_masks)[0]\n",
        "  #dense_layer = tf.keras.layers.Dense(64, activation='tanh')(seq_outputs)\n",
        "  outputs = tf.keras.layers.Dense(num_labels, activation='sigmoid')(seq_outputs)\n",
        "  \n",
        "  model = tf.keras.models.Model(inputs=[input_ids,input_masks], outputs=outputs)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEZz33bXoNTV",
        "outputId": "5590a48c-fa23-4304-cc11-8f91b7592f70"
      },
      "source": [
        "pt_trans_model = 'bert-base-uncased'\n",
        "num_labels = 1\n",
        "loss = 'binary_crossentropy'\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "\n",
        "eng_model = create_eng_transformer_model(pt_trans_model, num_labels, 80)\n",
        "eng_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 109483009   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 109,483,011\n",
            "Trainable params: 109,483,011\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iik4kmarq0_l"
      },
      "source": [
        "#make labels into numpy arrays, and prep x_val\n",
        "x_val = [eng_val_input, eng_val_masks]\n",
        "#turn labels into tensors\n",
        "y_train = np.asarray(y_train, dtype='int32')\n",
        "y_val = np.asarray(y_val, dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFHS3uq5q7OM",
        "outputId": "24727cdd-1252-4a7d-c99e-a587f6284214"
      },
      "source": [
        "#fit model to input data\n",
        "class_weight = {0: 1.,\n",
        "                1: 17.}\n",
        "with tf.device('/device:GPU:0'):\n",
        "  eng_model.fit([eng_train_input, eng_train_masks], y_train, validation_data=(x_val, y_val), epochs=3, batch_size = 32, class_weight=class_weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7222 - accuracy: 0.8925WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "188/188 [==============================] - 67s 359ms/step - loss: 0.7222 - accuracy: 0.8925 - val_loss: 0.3186 - val_accuracy: 0.9255\n",
            "Epoch 2/3\n",
            "188/188 [==============================] - 66s 353ms/step - loss: 0.4242 - accuracy: 0.9357 - val_loss: 0.2429 - val_accuracy: 0.9255\n",
            "Epoch 3/3\n",
            "188/188 [==============================] - 66s 353ms/step - loss: 0.2784 - accuracy: 0.9565 - val_loss: 0.3016 - val_accuracy: 0.9215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chf0XT1l1JiI",
        "outputId": "e0f1cc22-04da-4edf-f537-11d2b87b3a55"
      },
      "source": [
        "eng_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 109483009   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 109,483,011\n",
            "Trainable params: 109,483,011\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDD7_ML85jEq"
      },
      "source": [
        "eng_model.save_weights('/content/drive/MyDrive/Colab/saved_model/engBert_profner_weight_2.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM8P-d5dswuq"
      },
      "source": [
        "##Testing and Error Analysis on Bert Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvX3stkRuzrn"
      },
      "source": [
        "#import model if needed\n",
        "#eng_model.load_weights('/content/drive/MyDrive/Colab/saved_model/engBert_profner3.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFi5T7EFs2aB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2465be-59de-474a-a5f3-331be8d002a9"
      },
      "source": [
        "#make predictions on model\n",
        "eng_y_pred = eng_model.predict(x_val)\n",
        "eng_pred = assign_class(eng_y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoT6Gh7b4HRF",
        "outputId": "ae0daa41-449e-47eb-c557-1597e2af9df8"
      },
      "source": [
        "#see what is going wrong\n",
        "print(eng_pred[:20])\n",
        "print(type(val))\n",
        "eng_pred_dic = {} #index of tweet is key and values are predicted value\n",
        "for i, p in enumerate(eng_pred):\n",
        "  if y_val[i] != p:\n",
        "    eng_pred_dic[i] = {}\n",
        "    eng_pred_dic[i]['tweet'] = eng_val[i]\n",
        "    eng_pred_dic[i]['pred'] = p\n",
        "    eng_pred_dic[i]['true'] = y_val[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n",
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehUSldwO4jid",
        "outputId": "71a94bd4-4694-4362-8d29-c7b92970ffd9"
      },
      "source": [
        "print('incorrect classification in english model', len(eng_pred_dic.items()))\n",
        "print(eng_pred_dic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "incorrect classification in english model 157\n",
            "{6: {'tweet': '[\\'\"The initial mismanagement of #COVID19 has not been a failure of U.S. intelligence (which did warn policymakers), but has been one of the worst and most dangerous policy failures in U.S. history.\"\\\\n\\', \\'https://t.co/mkpU8YENcz\\']', 'pred': 1, 'true': 0}, 21: {'tweet': \"['🚑In Santander, an ambulance wasting precious time because of the far-right demonstration.\\\\n', '\\\\n', 'It serves as a metaphor for what certain irresponsible attitudes are implying in the midst of a pandemic, while working people are organizing to save lives.\\\\n', '#ElVirusSoisVoxotros https://t.co/7l2v2wfoJm']\", 'pred': 1, 'true': 0}, 41: {'tweet': \"['Today we have approved in the Council of Ministers the #COVID19 Fund for the Autonomous Communities. Endowed with 16,000 million euros, it is the largest disbursement of resources for the autonomous regions in the recent history of our democracy, reflecting our firm commitment to the autonomous State. https://t.co/uhl3NkUJJv']\", 'pred': 1, 'true': 0}, 50: {'tweet': '[\"#Coronavirus 🤣🤣🤣🤣🤣🤣🤣 If it weren\\'t for what it is.... By the way, I read a new concept today: *necropoliticians*. I\\'ve always referred to political necropoliticians, I\\'ll use the new concept. https://t.co/OGLRSa6X2S\"]', 'pred': 0, 'true': 1}, 59: {'tweet': '[\"He has made a parliamentary announcement from the automakers\\' headquarters \\\\U0001f928\"]', 'pred': 1, 'true': 0}, 63: {'tweet': '[\\'Belgian aristocrat takes a plane to Spain to throw an illegal party in Cordoba with 25 other people. Soon after, he develops symptoms and tests positive for COVID-19.\\\\n\\', \\'\\\\n\\', \"Don\\'t do like Prince Joachim and his rich friends. Protect yourself and others. https://t.co/7hgOfLUWIV\"]', 'pred': 1, 'true': 0}, 67: {'tweet': \"['Revista C8M 07. The coronavirus crisis from a feminist and trade union perspective. \\\\n', '\\\\n', 'https://t.co/bsPJsGE0pi https://t.co/Y0qVFu2kk4']\", 'pred': 1, 'true': 0}, 113: {'tweet': '[\\'I play Goonies for the first time. to my little daughter. As soon as it starts.\\\\n\\', \"- I\\'ve already seen it, Mom.\\\\n\", \\'- When? \\\\n\\', \\'- A day you were at work, from when you were little at home, from before the coronavirus.\\\\n\\', \\'.\\\\n\\', \\'Boom.\\']', 'pred': 1, 'true': 0}, 129: {'tweet': \"['@Blablab12345671 @namela7919 @LRsecret When there is a surplus of deaths when there are multi-year averages for a reason. The figures are the figures. Here in spain xq is intuited people q already dead by covid? X the average mortality of subsequent years. It includes the flu does not? Or is it that in previous years there was no flu?']\", 'pred': 1, 'true': 0}, 142: {'tweet': '[\\'📡 LIVE, appearance @desdelamoncloa of the Technical Committee monitoring the state of alarm by #COVID19, April 25, 2020.\\\\n\\', \\'\\\\n\\', \"#NowIt\\'sTowToFightTogether\\\\n\", \\'\\\\n\\', \\'https://t.co/bylIZUw9N2\\']', 'pred': 1, 'true': 0}, 143: {'tweet': \"['3 weeks with the counter of deaths due to coronavirus not updated and the assholes applauding like seals. Not even in Somalia, colleagues.']\", 'pred': 1, 'true': 0}, 151: {'tweet': '[\\'They promote pots and pans against applauding #healthcare, reject the extension of the #StateOfAlarm and at the same time defend a \"strong state\" against federalism. How would they limit mobility between territories so as not to spread the virus again?\\']', 'pred': 0, 'true': 1}, 153: {'tweet': \"['Lleida and Barcelona were in phase 3 for one (1) day. https://t.co/PS2t0xCKH8']\", 'pred': 1, 'true': 0}, 155: {'tweet': '[\"What\\'s the point of a one-hour flight offering bar service and the person less than five feet away from you taking off their mask for 20 minutes to eat some fries @vueling ? And while cinemas and theaters closed?\"]', 'pred': 1, 'true': 0}, 166: {'tweet': '[\"I swear, I don\\'t wish ill on anyone, but I wouldn\\'t mind if they had a car run over them.\\\\n\", \"And to those protesters who want freedom, fed up with confinement, blah blah blah...Please don\\'t buy masks, there are other people who really need them.\"]', 'pred': 1, 'true': 0}, 174: {'tweet': \"['✝️We regret to communicate the death of another colleague due to #COVID19. \\\\n', '\\\\n', '😢Hilario in second activity, stationed in #Segovia. \\\\n', '\\\\n', 'We convey our deepest condolences to their families, friends and colleagues. \\\\n', '\\\\n', 'Death is not the end🇪 https://t.co/Y0OhkmNcnN']\", 'pred': 1, 'true': 0}, 181: {'tweet': \"['Amazing.\\\\n', 'An immigrant was transferred to Extremadura at the request of the Government of Extremadura when the transit of people was prohibited.\\\\n', 'No check is made at the exit.\\\\n', '&gt;He tested positive for coronavirus in Navalmoral de la Mata and now it is not known where he is. https://t.co/A8e59XsZu4']\", 'pred': 0, 'true': 1}, 185: {'tweet': '[\"Today it\\'s time to read @rauljp93. Raul talks about what he is experiencing in Spain regarding COVID-19. I was really shocked. Not too much scaremongering, it\\'s simply a chronicle of what he has experienced. \\\\n\", \\'Read it, I promise it will be 3 minutes well spent. 👇\\']', 'pred': 1, 'true': 0}, 200: {'tweet': '[\\'\"Ayuso will have to compensate the canteen companies he suspended the contract to opt for Telepizza during the pandemic\" https://t.co/I9WGSWfwq2\\']', 'pred': 1, 'true': 0}, 243: {'tweet': '[\"The Junta de Andalucía paid in full confinement 99,000 euros to senior officials of PP and Cs for the rent of their \\'second homes\\' via @El_Plural https://t.co/5IzFZw8gWi\"]', 'pred': 1, 'true': 0}, 262: {'tweet': \"['It is published by @Edwanafrica and, although it is already available for purchase, shipments will be made when the confinement ends. By buying now, you will help the publisher in these difficult times. The copyright of the book will be transferred in full to the @Saharabubisher project.']\", 'pred': 0, 'true': 1}, 282: {'tweet': \"['The fact that the visible face of the fight against the pandemic says that the criteria for de-escalation are not public and neither is the scientific committee, makes me shudder. And that the disseminators do not cry out to heaven when this happens, too.']\", 'pred': 0, 'true': 1}, 288: {'tweet': \"['@policia @guardiacivil \\\\n', '\\\\n', '🆘 I thought #Madrid was still at STAGE ZERO but while some of us Spaniards continue to be confined in compliance with the Law, others endanger the spread of the virus #Covid19 🆘']\", 'pred': 0, 'true': 1}, 295: {'tweet': \"['The AD Magallon informs all its members that during Thursday, Friday and Saturday from ten to eleven thirty in the morning on the terrace of the market bar, will be delivered free of charge a mask to combat covid19 with the shield of our team.']\", 'pred': 1, 'true': 0}, 299: {'tweet': \"['@pnique @JALGUERRERO Look what he explained here: More than a cold , less than a flu ....\\\\n', 'As of today, this virus less than a flu has taken 30,000 people!!!! As to believe him because he says that the 8M was not a focus of contagion!!! https://t.co/bj9CizaPzY']\", 'pred': 1, 'true': 0}, 333: {'tweet': \"['Spain XXI Century \\\\n', 'The monarchy and the clergy, who had disappeared during the pandemic, are seen at a large solemn mass.\\\\n', 'As in the Middle Ages. https://t.co/sVYVGbxFsi']\", 'pred': 1, 'true': 0}, 342: {'tweet': \"['Imagine these sons of bitches in office']\", 'pred': 1, 'true': 0}, 349: {'tweet': \"['From Lleida They Express Their Opinion And How They Feel Regarding The Management Of COVID-19 In Prisons https://t.co/jKh4ZHgzGp https://t.co/4jAaw3L7Ik']\", 'pred': 1, 'true': 0}, 352: {'tweet': '[\"Urkullu presses Sánchez to bring 200 ETA prisoners closer ➡ TO AVOID THE PROPAGATION OF THE CORONAVIRUS. He assures that the family members\\' trips are a risk. The request comes after new attacks on political headquarters. By @xuxana https://t.co/2wUa83X0f8\"]', 'pred': 0, 'true': 1}, 353: {'tweet': \"['Hungary denies Santiago Abascal: they have not distributed masks to the entire population. The leader of VOX gave as an example of the management of the coronavirus the government of the far-right Viktor Orbán.\\\\n', 'https://t.co/VfuxSb4f6h']\", 'pred': 1, 'true': 0}, 370: {'tweet': \"['How is it possible that being #CLM one of the Autonomous Communities with more healthcare infected by #Covid19, @garciapage and @gobjccm even consider allocating aid to #Bullfighting instead of #Healthcare? #NoMásTorosCLM https://t.co/D3mWgzyKjQ']\", 'pred': 0, 'true': 1}, 372: {'tweet': \"['Four people die after drinking hand sanitizer used for coronavirus https://t.co/wPh3hLJ7m3 via @24clm']\", 'pred': 1, 'true': 0}, 378: {'tweet': \"['\\\\U0001f9f1Operations in Existing Buildings \\\\U0001f9f1 #OrderSND /385/2020, of May 2, amending SND/340/2020, of April 12.\\\\n', '👉 https://t.co/GDcfkaIs7T\\\\n', '\\\\n', '#descaling #COVID19 #works #Construction #Housing #Reforms #Rehabilitation #FincasAdministration #OrderSND #autonomous https://t.co/X92QE7JoTD']\", 'pred': 0, 'true': 1}, 440: {'tweet': '[\\'And if instead of asking for more help from \"Papa State\" for the increase in delinquencies, homeowners show their patriotism by forgiving rent to their tenants for as long as they have not been able to work because of the pandemic?         🤔 https://t.co/zPeHG8zh5r\\']', 'pred': 1, 'true': 0}, 482: {'tweet': \"['A missed opportunity😞 as a tourist attraction and security guarantee.\\\\n', '@jmfuentespila .@AmparoCoterillo .@JesusGoni .@vicentenieto8\\\\n', '#blueflags #beaches #COVID19 #santander\\\\n', 'https://t.co/4MNv7s3Qj7']\", 'pred': 1, 'true': 0}, 499: {'tweet': \"['Cockfighting during coronavirus season in Adra @guardiacivil @COPEAlmeria https://t.co/wWd8zHGg2r']\", 'pred': 0, 'true': 1}, 551: {'tweet': '[\\'@protestona1 And when they tell you that the mask with a beard.... \\\\n\\', \"the strange thing is that he doesn\\'t know it already being a doctor at Harvard.... (ah that his degree was not in medicine? And that he is not a doctor either? And that he went to Aravaca???? And now you will tell me that he got it without going to class, no shit!)\"]', 'pred': 1, 'true': 0}, 564: {'tweet': \"['Today the patriots are not demonstrating, neither are those who fight for Spain nor for freedom. Today those who invoke the fatherland and sell it, the bourgeoisie and the lords, those who intend to exploit us even more, are demonstrating.\\\\n', '\\\\n', '#CaravanaFaseLibertad #ElVirusSoisVoxotros (#CaravanaFaseLibertad #ElVirusSoisVoxotros)']\", 'pred': 1, 'true': 0}, 566: {'tweet': '[\"The word cold is misleading. It evokes cold, but infections are caused by pathogens, not thermometer degrees. If we don\\'t have flu in summer it is (in large part) because of more fresh air and ventilation. The coronavirus seems to act the same way https://t.co/xmC23GqcaX\"]', 'pred': 1, 'true': 0}, 620: {'tweet': \"['Even knowing that mistakes could have been made, our region was the first to obtain effective rapid tests that were distributed in a priority order to personnel exposed to the front line against Covid 19.\\\\n', '#CLMagainstCovid https://t.co/2UQfaXezXf']\", 'pred': 1, 'true': 0}, 625: {'tweet': \"['The Madrid Press Association categorically rejects that hoaxes are tackled by limiting freedom of information https://t.co/ist0VhUjhl #communiqué #COVID19']\", 'pred': 1, 'true': 0}, 640: {'tweet': \"['Nearly 200 Palestinian children are being held in Israeli prisons, most of them awaiting trial, in conditions that do not guarantee their protection against the coronavirus. Our colleague @clairenicoll knows their story well and tells it here\\\\n', 'https://t.co/InszWYlTXu']\", 'pred': 1, 'true': 0}, 651: {'tweet': '[\\'That day there were movie theaters, rallies, the Metro, fairs at IFEMA....\\\\n\\', \\'\\\\n\\', \\'But the extreme right and the far right only put the focus on one event:\\\\n\\', \\'The #8M feminist demonstration.\\\\n\\', \\'\\\\n\\', \\'\\\\U0001f9d1🏼⚖️News: Judge rules out 8M and virus expansion.\\\\n\\', \\'\\\\n\\', \"#NowIt\\'sTowToFightTogether \\\\n\", \\'\\\\n\\', \\'https://t.co/ph581ZcMlr\\']', 'pred': 0, 'true': 1}, 662: {'tweet': '[\"The Balearic Public Prosecutor\\'s Office files in record time the complaint of Vox on the impact of the coronavirus in nursing homes https://t.co/YDk4jdroK3\"]', 'pred': 1, 'true': 0}, 665: {'tweet': \"['#ULTIMAHORA \\\\n', '\\\\n', 'Specifically, every time you go to WORK because the registry activity has been declared essential by the RDL 8/2020.\\\\n', '\\\\n', 'Greetings from the Registrar \\\\n', '\\\\n', 'Congratulations on the scoop\\\\n', '\\\\n', '🤦♂️🤦♂️🤦♂️🤦♂️ https://t.co/1STvDfuYxs']\", 'pred': 1, 'true': 0}, 679: {'tweet': '[\\'You can see how \"useful\" it was to support the toxic couple to extend the state of alarm: for Sanchez to behave like a kinglet, distribute posts and money among his cronies and avoid democratic control. https://t.co/Bu9yCF7LBj\\']', 'pred': 1, 'true': 0}, 680: {'tweet': \"['@IdiazAyuso @eruizescudero ONLY YOU WILL BE RESPONSIBLE FOR THE SEQUELS AND DEATHS BY CORONAVIRUS\\\\n', '\\\\n', 'Primary Care, key to detecting new COVID-19 outbreaks, does not have the necessary reinforcements to be efficient https://t.co/bHPcm6z7HT via @eldiarioes']\", 'pred': 1, 'true': 0}, 683: {'tweet': '[\\'\"The government plans to limit the number of students per classroom to 15 next school year.\"\\\\n\\', \\'\\\\n\\', \"It took a pandemic to set a target of Finland\\'s average student-teacher ratio.\\\\n\", \\'\\\\n\\', \"That\\'s how we\\'re doing 🙄\"]', 'pred': 0, 'true': 1}, 740: {'tweet': \"['Capellanes que no falten aunque falten #rastreadores There is no way #Ayuso understands what a non-denominational state means nor what a public health system demands. Reports @AlvaroSanCas']\", 'pred': 0, 'true': 1}, 749: {'tweet': \"['Healthcare breaks the law by concealing coronavirus suppliers and contracts https://t.co/DA22lKFmZw \\\\n', '\\\\n', '@elconfidencial \\\\n', '#COVID19 #CoronavirusSpain #YomeQuedoEnCasa \\\\n', '#TransparencyCovid19 \\\\U0001f9a0⚠️']\", 'pred': 1, 'true': 0}, 759: {'tweet': '[\\'When there were already cases in Madrid and a local uncontrolled outbreak (Valdemoro) @IdiazAyuso pointed out, ignoring WHO and China. \\\\n\\', \\'\\\\n\\', \\'\"The most dangerous thing is fear, more than the virus itself.\"\\\\n\\', \\'\"The symptoms it leaves behind are less than those of the flu.\"\\\\n\\', \\'\\\\n\\', \\' https://t.co/vAJvRwtJjv\\']', 'pred': 1, 'true': 0}, 761: {'tweet': '[\"The Public Prosecutor\\'s Office of Castilla y León has opened proceedings to investigate the Junta\\'s decision not to transfer elderly people infected by coronavirus in nursing homes to hospitals. https://t.co/0Ylmqfxkbk\"]', 'pred': 1, 'true': 0}, 782: {'tweet': \"['noticiero cofrade: Coronavirus in Malaga: Those cured are twice as many as those ... https://t.co/HrbOSorXC7']\", 'pred': 0, 'true': 1}, 784: {'tweet': \"['#COVID19 and bank debtor protection https://t.co/FwvxHL9Fdg Discussed by @mateojuangomez on #BlogDerechosConsumidores.']\", 'pred': 1, 'true': 0}, 787: {'tweet': \"['I thought I had seen it all with the mask issue but seeing a lady with an Fpp2 mask and underneath a homemade mask made of a transparent plastic bag like the ones used for bread IS ANOTHER LEVEL.']\", 'pred': 1, 'true': 0}, 794: {'tweet': \"['Iglesias in the supermarket, without a mask, without gloves, and with an escort. All an example of the level of this Government that does not even know what shame is. And meanwhile the friendly and subsidized press will make us reports and covers and will continue blaming the opposition... https://t.co/q3i53Rhwou']\", 'pred': 1, 'true': 0}, 798: {'tweet': '[\"Murcia turns green in tribute to the \\'indispensable\\' of the pandemic\\\\n\", \\'https://t.co/OnfyopAlhj\\']', 'pred': 1, 'true': 0}, 799: {'tweet': \"['We continue with real examples. \\\\n', 'Lady, around 80 years old, diagnosed with COVID19, very severe. She did not go out of the house for fear of COVID19, she did everything correctly. \\\\n', '\\\\n', 'What about contagion? \\\\n', '\\\\n', 'His grandson, around 20 years old, after a party with known outbreak. He asymptomatic infection']\", 'pred': 1, 'true': 0}, 806: {'tweet': '[\\'More than one million formal jobs have been lost, industrial activity and tourism have plummeted, thousands of companies have closed, and López Obrador responds with a \"Decalogue\" that includes distancing ourselves from consumerism, not eating junk food, exercising and seeking a path of spirituality.\\']', 'pred': 1, 'true': 0}, 819: {'tweet': \"['But if the tests do not arrive, if it turns out that we do not have national manufacturing and we depend on intermediaries (who know them at home) if in the case of the mask it is being demonstrated that the best option, given the lack of material, was the reusable one even if it is a scam.']\", 'pred': 0, 'true': 1}, 823: {'tweet': '[\\'@DanielPeribanez @Resistir18 It\\\\\\'s a matter of masks and distancing and little else, like in Germany. They\\\\\\'ve handled it just fine without confinement. People have to be responsible, that\\\\\\'s all. But \"daddy state\" can\\\\\\'t decide. The Constitution decides!!!\\']', 'pred': 1, 'true': 0}, 831: {'tweet': \"['!!!️VOX and PP competing to be the naughty children of Congress while the citizenry strictly complies with the responsibility of confinement.\\\\n', '\\\\n', 'It is not only that they are not up to the task of the people, it is that they put the workers of the Congress at risk. Rectify. 👇 https://t.co/RMUffSyzFb']\", 'pred': 1, 'true': 0}, 838: {'tweet': \"['📢#webinarGRATUITO \\\\n', '\\\\n', '👉Occupational Risk Prevention Management and preventive measures to adopt in the face of #COVID19 in Agro-food Industries and Wineries.\\\\n', '\\\\n', '📲 https://t.co/D0vaHSzMZx\\\\n', '\\\\n', '🗓️2/JUN.\\\\n', '⌚️12.00\\\\n', '\\\\U0001f9d1💻Adolfo Pavón and Raquel Sáenz\\\\n', '\\\\n', '#teletraining\\\\n', '#industrialengineers\\\\n', '#PRL https://t.co/SscE3GnSSS']\", 'pred': 0, 'true': 1}, 853: {'tweet': '[\"Of @IdiazAyuso\\'s apartment, of the kitten with coronavirus, or of the \\'fachas\\' pans in Madrid you did hear about.\\\\n\", \\'\\\\n\\', \\'From the Court of Auditors detecting four days ago 25 errors worth 9 billion in the 1st fiscal year of the @SanchezCastejon Government, no.\\\\n\\', \\'\\\\n\\', \\'Ask yourself why\\']', 'pred': 1, 'true': 0}, 862: {'tweet': \"['😷 A release carried out as thanks for the great health work done to face the Covid19 pandemic and attended by representatives of the five health departments corresponding to the hospitals of València.\\\\n', '\\\\n', 'News: https://t.co/DYy1D3bNvZ https://t.co/0CWN6rVrHS']\", 'pred': 1, 'true': 0}, 892: {'tweet': \"['@HeiwaPaz @PoderJudicialEs Espero q al ser grupo de riesgo esté en casa.Sus colegas d GALAPAGAR se han dedicado a expandir virus y BULOS, y a condenar a muerte a 22.157 PERSONAS, por su nefasta gestión. GOOD NIGHT,IF YOU CAN SLEEP']\", 'pred': 1, 'true': 0}, 894: {'tweet': \"['A text from 1348 by the Almerian intellectual Al Bilyani found in Cairo defends the need for confinement to prevent the spread of the Black Death https://t.co/qv3ph3q6NU']\", 'pred': 1, 'true': 0}, 900: {'tweet': '[\\'On the left, bathers. On the right, counseling for those affected by the pandemic and abusive rents.\\\\n\\', \\'\\\\n\\', \"Let\\'s see if you can guess who has been identified and harassed by the police. For the third week in a row. https://t.co/VRL10HUkWF\"]', 'pred': 1, 'true': 0}, 902: {'tweet': '[\"If you were thinking about it, don....\\'t hesitate! REGISTER! The 7th and 9th of July and you will receive the official Sanitary Mask of the Club!!!! Do not miss it!!!.... https://t.co/9rpU96Va5i\"]', 'pred': 1, 'true': 0}, 904: {'tweet': \"['Cadiz is full of precariousness. \\\\n', '#strikerentscadiz #suspensionofrents #COVIDー19 #Environment #Cádiz #cadiz https://t.co/Tr9ift7mQK']\", 'pred': 1, 'true': 0}, 915: {'tweet': \"['It hurts my soul to hear this from good friends .....AND great wonderful professionals who give their all every day. The darn #coronavirus has come to turn everything upside down.']\", 'pred': 1, 'true': 0}, 923: {'tweet': \"['🚨Lleida sees up to 600 covid suspects a day in the outpatient clinics https://t.co/E4hBFRakXV']\", 'pred': 1, 'true': 0}, 924: {'tweet': '[\\'\"Lessons from an 84-year-old coronavirus survivor\" https://t.co/pu1a361Es7\\']', 'pred': 1, 'true': 0}, 932: {'tweet': \"['Chocolates Valor has raised the salary of its staff by 20% in the middle of the pandemic, they are the same people who 7 years ago rescued little bones because they were going to take the production to Poland, and have donated 300k euros for the fight against covid 19.\\\\n', '\\\\n', 'To take it into account in the next purchase. https://t.co/yf0ShxazrU']\", 'pred': 1, 'true': 0}, 943: {'tweet': \"['Small boiler...']\", 'pred': 1, 'true': 0}, 989: {'tweet': \"['Justice condemns what Basque society knows and lives in first person: that the PNV, Urkullu and his government do not know how to manage, they only know how to share power among friends. The Ertzaintza has been unprotected before the #COVIDー19 . It is a scandal and real irresponsibility https://t.co/tRzeJTznPq']\", 'pred': 0, 'true': 1}, 991: {'tweet': \"['This is our year, gentlemen']\", 'pred': 1, 'true': 0}, 998: {'tweet': \"['#CCOO denounces before the #DefensorDelPueblo the actions and serious breaches by different Prevention Services of #SERMAS @fssccoo @CCOOMadrid #COVID19 #ccoontigodíaadía https://t.co/orffMeN8Cr']\", 'pred': 1, 'true': 0}, 1032: {'tweet': \"['Wonderful, fantastic, without words. Next time, this energumen will learn not to be such an asshole.']\", 'pred': 1, 'true': 0}, 1040: {'tweet': \"['COVID-19 Protocol proposed by the Sociedad Argentina de Aviacion for General Aviation. https://t.co/v07fer0f58']\", 'pred': 1, 'true': 0}, 1053: {'tweet': '[\\'I love Mr. @GLlamazares\\\\n\\', \\'He \"threatens\" to silence those who cheer against the government over the pandemic as he finds it undemocratic and irresponsible.\\\\n\\', \\'\\\\n\\', \\'Yours in 2014 with Ebola was responsible and democratic or vice versa? 🤔 https://t.co/o9zHBlzqh3\\']', 'pred': 1, 'true': 0}, 1085: {'tweet': '[\\'One thing that may take this pandemic ahead and erase it from this Spain of mine that I love so much is the figure of the \"heroic bad guy\" who goes to the office while he is in a hurry to be able to say \"nothing, I\\\\\\'ve taken a brake-dol and I\\\\\\'ve come because I\\\\\\'ve got a lot of trouble\".\\']', 'pred': 1, 'true': 0}, 1087: {'tweet': '[\\'📰🗞️ Extra extra @fecstlm premieres its \"Teatro confinado\" You have vip pass (link) to the show in the article     Congratulations to the whole cast! @Mercetomelloso 🎞️🎭 https://t.co/3DKZXljVj1 #Theater #theatre #schooltheater #Covid_19\\']', 'pred': 1, 'true': 0}, 1135: {'tweet': \"['Green light to the compensation for highway concessionaires for the drop in traffic due to the confinement. Another bailout for Florentino Pérez. \\\\n', '\\\\n', 'https://t.co/Wfla4h3zon']\", 'pred': 1, 'true': 0}, 1140: {'tweet': \"['We will never forget this message from our great leader']\", 'pred': 1, 'true': 0}, 1143: {'tweet': \"['Torra announces the perimetral confinement of El Segrià (Lleida) due to the increase of contagions by coronavirus https://t.co/hZsmp5EVIo']\", 'pred': 1, 'true': 0}, 1153: {'tweet': '[\\'They found that all patients with COVID-19 had a robust CD4, or \"helper\", T-cell response that aids in antibody production. Almost all patients had produced virus-specific CD8 or \"killer\" T cells.... 👇\\']', 'pred': 0, 'true': 1}, 1159: {'tweet': '[\\'My uncle Gustavo opened a chicken restaurant on Grecia Ave. and because of COVID-19 he is about to lose all his assets.\\\\n\\', \\'\\\\n\\', \\'Could you give me a RT to promote \"Los Pollos Hermanos\"?\\\\n\\', \\'\\\\n\\', \\'We love you uncle Gus! Cheer up, everything will be ok 🤗\\\\n\\', \\'\\\\n\\', \"Let\\'s support the local entrepreneur 👍🏻 https://t.co/2aTdM9s4uU\"]', 'pred': 0, 'true': 1}, 1188: {'tweet': \"['YouTube: Les Luthiers publishes four new shows on its channel due to quarantine | COVID-19 | Argentina | RPP Noticias https://t.co/aOeXz6Ygmi']\", 'pred': 1, 'true': 0}, 1190: {'tweet': '[\"The electronic notary\\'s office ensures the notarization by videoconference and is in the general interest:\\\\n\", \\'\\\\n\\', \\'It broadens rights for citizens.\\\\n\\', \\'\\\\n\\', \\'Saves costs and travel time.\\\\n\\', \\'\\\\n\\', \\'Includes people made vulnerable by the pandemic.\\\\n\\', \\'\\\\n\\', \\'It only excludes corporate interests.\\']', 'pred': 1, 'true': 0}, 1197: {'tweet': '[\"If the @DalaiLama has been able to lead from afar despite 61 years of exile, what\\'s a few months of confinement for business leaders? With @AngelesDelgado_ @SilviaFores @Jaimeasnai @Jlpascualpe My report today in @elEconomistaes. \\\\n\", \\'https://t.co/0SWwauuSZQ\\']', 'pred': 1, 'true': 0}, 1226: {'tweet': \"['Covid-19 crisis makes it clear that the Monarchy is expendable\\\\n', '\\\\n', 'The health crisis is showing how the current Head of State lives with his back turned to the concerns of the citizens and does not represent the people. \\\\n', '\\\\n', 'https://t.co/rfxnWMa0DP\\\\n', '#MonarchyDemeritus ❤💛💜']\", 'pred': 1, 'true': 0}, 1228: {'tweet': \"['Valencia phase 1: beaches are full of bathers despite the increase in the Covid-19 infection rate https://t.co/SRUZ9AKeeB via @ABC_CValenciana']\", 'pred': 1, 'true': 0}, 1257: {'tweet': '[\\'Fernando Simón warns that it is necessary to prepare for \"a possible upturn\".\\\\n\\', \\'\\\\n\\', \\'I was not prepared to put up with such a beggar.\\\\n\\', \\'\\\\n\\', \\'Test, test, test, test, test... from the first day of confinement.\\\\n\\', \\'And not this disaster.\\\\n\\', \\'This government is a political-scientific horror.\\']', 'pred': 1, 'true': 0}, 1271: {'tweet': '[\"It\\'s a disgrace!\\\\n\", \\'Neither #EquiparacionYa, nor cataloging them as risk personnel to have priority in the tests, nor recognize contagions as a result of work and the most bleeding is q still insinuates the superfluous expenditure.\\\\n\\', \\'#WeAreNotSuperfluousSpending @sanchezcastejon\\\\n\\', \\'@jusapol https://t.co/WikcyJNfRs\\']', 'pred': 1, 'true': 0}, 1282: {'tweet': \"['Many young people think they are immortal to the virus, just look at their behavior ... Parties botellones meetings.... the bad thing is that we infect many people who are not to blame d s irresponsibility. First s family ... With 30 years should be + responsible\\\\n', 'https://t.co/RaNz1IBS0P']\", 'pred': 1, 'true': 0}, 1297: {'tweet': '[\"We uncovered the alleged specialist Miguel Lacambra, whom major opinion leaders have been retweeting for days. In reality it is a set-up, a fake account created exclusively to wash the image of the government\\'s management of the #COVID19 crisis.\\\\n\", \\'https://t.co/Kc9om7h2pY\\']', 'pred': 1, 'true': 0}, 1300: {'tweet': '[\"My mother after years of unemployment started a business just before the covid and these months have been horrible for her, the school season is starting and we are recovering, it doesn\\'t cost you anything to come here for the photocopies of the uni or anything else, we also bring on request :) https://t.co/ber1PY3kSu\"]', 'pred': 0, 'true': 1}, 1308: {'tweet': \"['⚠ #UltimaHora Catalonia confines the region of Segrià, in Lleida, due to the increase of coronavirus contagions https://t.co/BHgaAxUAyg']\", 'pred': 1, 'true': 0}, 1326: {'tweet': '[\"Hi, I\\'m progre: it\\'s not true that we laughed about the epidemic before 8-M, nor that we induced people not to protect themselves, nor that we avoided taking action until after the manifa. It\\'s all a conspiracy of Opus, the legionaries of Christ and 2 tricorns, hahaha #cómomolo\"]', 'pred': 0, 'true': 1}, 1327: {'tweet': \"['@NicoVBravo Thanks, I just have the doubt (a few days ago) that if all cases of flu, and death by other factors correspond to covid.']\", 'pred': 1, 'true': 0}, 1331: {'tweet': \"['Checo Perez first candidate to receive a points deduction for coronavirus in @ligasordidaf1']\", 'pred': 1, 'true': 0}, 1363: {'tweet': '[\"Indetix, Amancio Ortega\\'s Zara made profits in 2019 of 4000 million €, now because of a global pandemic loses 400 million, closes 300 stores and lays off thousands of employees. Shameless is not enough.\"]', 'pred': 1, 'true': 0}, 1373: {'tweet': '[\\'BEWARE of the so-called \"Covid fee\". We explain why it is illegal for a retailer to charge you an extra fee for disinfection COVID-19 https://t.co/UV9qlB620S\\']', 'pred': 1, 'true': 0}, 1385: {'tweet': \"['@Ana99544117 According to voxarians they are lazy, but they also think that waving flags fixes a global pandemic 🤷♀️ https://t.co/wMnnEPXP6Q']\", 'pred': 1, 'true': 0}, 1406: {'tweet': '[\"PSOE and Podemos raised my minimum wage from 700 to 900 euros, they raised my father\\'s pension, they managed that my sister who became unemployed during the pandemic with a child was not evicted from her house, the most disadvantaged people will receive the minimum income THANK YOU THANK YOU THOUSAND!\"]', 'pred': 0, 'true': 1}, 1418: {'tweet': '[\\'\"Sanchez and Illa are denounced before the Anti-Corruption Prosecutor\\\\\\'s Office for embezzling more than 50 million in the COVID-19 crisis\", by @miguelpr83 https://t.co/H9aqgOgGGC via @libertaddigital\\']', 'pred': 1, 'true': 0}, 1427: {'tweet': '[\"Ayuso\\'s harassment of Telemadrid is not only aimed at subjecting its editorial line to his whims. He also wants to shower money on an opinionated manger as Esperanza Aguirre did in her time. And he takes advantage of Covid-19 to do it from behind. https://t.co/ZiCHYKkOvW\"]', 'pred': 1, 'true': 0}, 1439: {'tweet': '[\\'\"I get the message that we\\\\\\'re all in this together, but we celebrities don\\\\\\'t have the same battle as a stay-at-home mom who\\\\\\'s out of work because of #COVID19 and has to struggle to provide for her kids.\" -Lady Gaga\\']', 'pred': 0, 'true': 1}, 1443: {'tweet': '[\"let\\'s see Hugo hasn\\'t done it wrong by uploading a story that he is going to peel his hair for God\\'s sake, who is doing it wrong is the people who don\\'t give a damn about anything and stand at the door of a hairdresser\\'s in the middle of a pandemic to take pictures of themselves when they already have a photo of themselves.\"]', 'pred': 1, 'true': 0}, 1446: {'tweet': \"['Today we are defending a RDL that reinforces the commitment to the fight against male violence.\\\\n', 'On behalf of the confederal group, I will intervene to say that it is pertinent and necessary and that we want to be alive and free.']\", 'pred': 1, 'true': 0}, 1447: {'tweet': \"['Why was Covid-19 underestimated? I spent Holy Week trying to answer that question. Meanwhile, 2 colleagues and friends of the Faculty of Psychology of the @uam , Antonio Pardo and Fernando Pelaez died from the virus. This analysis is dedicated to them: https://t.co/hYnpy5WgVp']\", 'pred': 1, 'true': 0}, 1484: {'tweet': \"['Faithful to the Lasallian spirit, the A TEAM and assistants of @Lasallemelilla collaborate as many times in supplying the Bank so that we can provide service to many families who demand it.\\\\n', 'Thank you very much\\\\n', '#Imagina #SomosLaSalle #TuAlegríaLaMia #EsteVirusLoParamosUnidos https://t.co/ocGO1yGq53']\", 'pred': 1, 'true': 0}, 1487: {'tweet': '[\"1/9 I just took a look at the study that (supposedly) shows traces of COVID-19 in Barcelona\\'s sewage as early as May 2019. I have no NPI in biology (I am a physicist), but from what I have read I recommend caution. I explain in this THREAD:\"]', 'pred': 1, 'true': 0}, 1500: {'tweet': \"['@publico_es @JairoExtre Right wing manipulated followers will never admit the figures of 2100 less sanitary and 3000 less beds in the community of Madrid cost lives just like the virus.']\", 'pred': 0, 'true': 1}, 1507: {'tweet': \"['Neighbors of #Sanse, welcome to phase 1!!!\\\\n', '\\\\n', 'Now more than ever it is important not to lower our guard and to continue to maintain all preventive measures and to use masks when it is not possible to maintain the interpersonal distance of 2 meters.\\\\n', '\\\\n', '#SanseHealth https://t.co/3okerP7vrZ']\", 'pred': 1, 'true': 0}, 1511: {'tweet': \"['Not to mention that I do not see Indra, the creators of the Renfe website, setting up in three months a highly advanced and Orwellian surveillance system to spy on our whatsapps.']\", 'pred': 1, 'true': 0}, 1512: {'tweet': '[\\'Outbreak #COVID19 Basque hospitals \\\\n\\', \\'\\\\n\\', \\'🗣️Nekane Murga: \"Tests will be conducted on the entire staff.\" \\\\n\\', \\'🗣️N.M. 10/06: \"Everything under control\".\\\\n\\', \\'🗣️N.M. 12/06: \"Everything under control\".\\\\n\\', \\'🗣️N.M. 06/15/06: \"Everything under control\".\\\\n\\', \\'\\\\n\\', \\'H.Basurto: 2,937 tests\\\\n\\', \\'H.Txagorritxu: 136 test, yes 136.\\\\n\\', \\'\\\\n\\', \\'#PorUnaEuskadiParaLaClaseObrera (ForAEuskadiForTheWorkingClass)\\']', 'pred': 1, 'true': 0}, 1515: {'tweet': '[\"#2May The brothers of the Cuban Medical Mission in #Aragua together with the People\\'s Power, tour the hamlets of the #Tovar municipality to carry out the screening of #COVIDー19. An act of nobility that reflects the solidarity and humanity of our peoples #FlexibilizaciónConDisciplina https://t.co/DoTM1I3tDt\"]', 'pred': 1, 'true': 0}, 1548: {'tweet': '[\"The strange \\'disappearance\\' of Princess Leonor during confinement https://t.co/DLViiuq7JG\"]', 'pred': 1, 'true': 0}, 1563: {'tweet': '[\\'https://t.co/AXDJue9NMV\\\\n\\', \\'Cristina Cifuentes denounced before the Public Prosecutor\\\\\\'s Office the tricks of Aguirre and her \"frogs\" with the City of Justice.\\\\n\\', \"That\\'s why they went after Cifuentes.\\\\n\", \\'The PP mafia.\\\\n\\', \\'Today they take there those who have died of coronavirus and cannot fit in the Ice Palace.\\\\n\\', \\'#COVID19\\']', 'pred': 1, 'true': 0}, 1568: {'tweet': '[\"@Choms @AstAstrea @elEconomistaes Read the enlightened article.... CASUALTIES AFTER GOVERNMENT ANNOUNCEMENT!!!! Post Covid! But of course... a podemite garbage eater who\\'s going to read... you are to repeat like parrots... https://t.co/ZdGOpKFZKs\"]', 'pred': 1, 'true': 0}, 1578: {'tweet': '[\"Japan\\'s professional soccer league will resume in a month\\'s time with matches between the geographically closest teams and will later be extended to other areas, among other safety measures to prevent COVID-19 contagion, organizers announced.\"]', 'pred': 0, 'true': 1}, 1580: {'tweet': \"['The stick is not so much to the Generalitat as to the health and life of the citizens of Lleida.\\\\n', '\\\\n', 'The Spanish prosecution and judiciary have handed down a sentence that will cause deaths.\\\\n', '\\\\n', 'The Spanish State has blood on its hands again.\\\\n', '\\\\n', 'Until when?']\", 'pred': 1, 'true': 0}, 1583: {'tweet': \"['Come on! Come on! That in Aragon we have a lot of supportive citizens and companies willing to collaborate #micro patronage @FundlaCaixa @SoydeZaragoza @IbercajaSocial @heraldoes @periodicoaragon     \\\\n', 'Please spread the word!']\", 'pred': 1, 'true': 0}, 1595: {'tweet': \"['Efectivos de la .@2ciadesurmerida, en el Mcpio. Alberto Adriani, practiced the retention of 600 cases of beer, for a total of 4,480 Lts, to a cddno. for not complying with the Presidential Decree, in the prevention of #Covid-19. A/o of the MP. #27jun #Fanb #PeridismoEmancipador https://t.co/5jo53k1tNX']\", 'pred': 1, 'true': 0}, 1629: {'tweet': \"['Low wages make it easier for more people to go to work with symptoms of #COVID19. And more than 70% of the salaried population has worked without protective measures. Article on the results of the COTS survey in porExperiencia magazine.\\\\n', 'https://t.co/NGR2Cwbu0D https://t.co/henKhki2iz']\", 'pred': 0, 'true': 1}, 1642: {'tweet': \"['https://t.co/nG2g92oKfo 🐉 Dragon Ball GT🐉 by T.Oda. Video 4 that I record during confinement in order to brighten up your day a bit.\\\\n', '#clarinetist #clarinet #clarinet #clarinet #clarinet #dragonball #goku #dragonballgt #4clarinet #clarinetquartet #soundtracks #yamahaclarinet #yamahaclarinets']\", 'pred': 1, 'true': 0}, 1672: {'tweet': '[\\'🔴 LAST MINUTE 🔴 LAST MINUTE 🔴 LAST MINUTE 🔴 LAST MINUTE\\\\n\\', \\'\\\\n\\', \\'Three counties of Huesca regress to phase 2 after several outbreaks were detected in the province. Sanidad says that the Zaidín outbreak is the only one of concern at this time. \\\\n\\', \\'\\\\n\\', \"It\\'s told by @carloscondetv on #LaMañanaTVE ➡ https://t.co/vFm9cY5pK1 https://t.co/aBkuViNrfY\"]', 'pred': 1, 'true': 0}, 1679: {'tweet': \"['The #PoliciaNacional DOES NOT FINE for carrying the flag of Spain 🇪🇸.\\\\n', 'The #PoliciaNacional DOES NOT FOLLOW ILLEGAL ORDERS of any Government of the day.\\\\n', 'The #PoliciaNacional ensures compliance with the law and your safety.\\\\n', '#StateOfAlarm\\\\n', '#TogetherWeWillAchieve\\\\n', '#StayAtHome https://t.co/YVFoT3dLVs']\", 'pred': 0, 'true': 1}, 1680: {'tweet': \"['That in a pandemic, where you have to fight against the unknown, with limited resources, without protective equipment, where resources have multiplied like loaves and fishes, although not the professionals, you want to blame those who risk their lives is not logical.']\", 'pred': 1, 'true': 0}, 1683: {'tweet': \"['My 14 year old daughter tested positive for Covid, she has asthma. According to the tests, she is in the first stage. Can you help me with a prayer and your good wishes? \\\\n', 'Thank you 🙏🏻']\", 'pred': 1, 'true': 0}, 1692: {'tweet': \"['Movistar, the favorite operator of Spaniards to be connected in times of Covid, according to the consulting firm Kantar https://t.co/475MmWvqzV via @Servimedia']\", 'pred': 1, 'true': 0}, 1696: {'tweet': '[\\'This nymph has not respected the confinement, yesterday she left her house and perched on a balcony of Paseo Juan Carlos I in Las Delicias.\\\\n\\', \"I\\'m sure its owner would love to get it back, @PoliciaVLL has the details of the person who has custody of it.\\\\n\", \\'Give RT to see if it reaches him! https://t.co/SDvl8uuEeD\\']', 'pred': 1, 'true': 0}, 1700: {'tweet': \"['Ok, I have been looking at the COVID Radar issue, specifically why it uses GPS if they are not supposed to track our location. TL:DR is justified and I think we should all use the app. Let me explain. Inside THREAD']\", 'pred': 1, 'true': 0}, 1707: {'tweet': \"['Responsible and creative leadership in times of COVID-19 https://t.co/0uVfeeIwW5']\", 'pred': 1, 'true': 0}, 1754: {'tweet': \"['❌👑 Today we must remind Juan Carlos and Felipe that we must be extremely hygienic against all types of viruses, bacteria and parasites.\\\\n', '\\\\n', '#ParasitesOut\\\\n', '#ForACountryForTheWorkingClass https://t.co/lEcJHF3q95']\", 'pred': 1, 'true': 0}, 1761: {'tweet': \"['Two months ago I suggested that children do NOT suffer from severe Covid-19 because their endothelium is healthy.... These authors say the same thing... https://t.co/ZFvqhLbHLZ']\", 'pred': 1, 'true': 0}, 1764: {'tweet': \"['Oh, Sebastian now discovers that this socialist government is a bunch of privateers.']\", 'pred': 0, 'true': 1}, 1791: {'tweet': '[\\'Simón: \"There will only be isolated infections\". Result: 285,000 infections.\\\\n\\', \\'\\\\n\\', \\'Sánchez: \"We will leave no one behind\". Result: Spain leads unemployment in the EU.\\\\n\\', \\'\\\\n\\', \\'Calviño: \"The coronavirus will have insignificant impacts\". Result: our GDP falls by 18.5%.\\\\n\\', \\'\\\\n\\', \\'Applaud seals.\\']', 'pred': 0, 'true': 1}, 1809: {'tweet': '[\"Today when I went to Lidl (Sweden) with mask and gloves the same thing happened to me as always, the guy in the back didn\\'t keep his distance. I told him to move, he did but not enough, called me a clown and sent me back to my country. guess what happened? I got kicked out of Lidl. https://t.co/nO4NmxhlAy\"]', 'pred': 0, 'true': 1}, 1810: {'tweet': \"['Says Pitonisa Monasterio that she knew since October 6, 2019 what was coming our way with the #coronavirus.\\\\n', 'One and a half months before the first case in China and three in Spain.\\\\n', 'She lies and stays so wide.\\\\n', 'This is VOX \\\\n', 'Some cheeky people. https://t.co/NrWkc0YeeK']\", 'pred': 0, 'true': 1}, 1824: {'tweet': \"['Así manipula los datos @pablocasado_ y como él, @juanma_moreno , @IdiazAyuso y todas las CCAA dirigidas por los @populares Ahora pretende el Presi de @AndaluciaJunta q @jesusraguirre , quien nefastamente gestionó la listeriosis, q gestione el #COVID19 Dios!!!! https://t.co/L6RyAwSC0o']\", 'pred': 0, 'true': 1}, 1826: {'tweet': '[\\'Even #PopeFrancisco speaks for that Spanish opposition that now says #NoAlEstadoDeAlarma.\\\\n\\', \\'and yesterday called for more drastic measures.\\\\n\\', \\'Yes, @Pontifex_es calls for unity among politicians. \\\\n\\', \\'In the #pandemic and says:\\\\n\\', \\'\"because unity is superior to conflict\" https://t.co/sg03OA1RgC\\']', 'pred': 1, 'true': 0}, 1841: {'tweet': '[\\'Oh, I forgot. I have been told that tomorrow @sanchezcastejon will do his \"Aló Presidente\" with his new anti-Spanish mask. Good night. https://t.co/KlrrYx7pSm\\']', 'pred': 0, 'true': 1}, 1843: {'tweet': \"['Apple iPhone 11 (64 GB) - in Yellow\\\\n', 'Price: 759,00 € FREE shipping. V\\\\n', 'You save: 50,00 € (6%)\\\\n', '\\\\n', 'CLICK HERE https://t.co/QxY0MoJ87c\\\\n', '\\\\n', 'MORE OFFERS HERE https://t.co/FeEb0yma6M\\\\n', '\\\\n', '#megaoffers #allaboutmobile #SVDebateFinal #CanalOT8J #COVIDー19 #Phase1 #Phase2 #Phase3 #telework https://t.co/FkWO9urgYm']\", 'pred': 0, 'true': 1}, 1856: {'tweet': \"['YEARS, NEGOTIATIONS, PROMISES...\\\\n', '\\\\n', 'After 37 years of democracy, the Urban Hygiene personnel have their own module equipped with toilets, showers and hand washing facilities for personal hygiene. Once they finish their working day, they will return home without any kind of virus... https://t.co/YhqHMt3fUD']\", 'pred': 1, 'true': 0}, 1910: {'tweet': \"['#InVideo📹 | Vicepdta @drodriven2: Current condition of patients with Covid-19.\\\\n', '\\\\n', '✅ 2,769 Asymptomatic\\\\n', '✅ 420 Mild acute respiratory failure.\\\\n', '✅ 5 Moderate acute insufficiency.\\\\n', '✅ 9 Severe acute respiratory failure in ICU.\\\\n', '✅ 39 Deceased\\\\n', '\\\\n', '#PreventiveConsciousness https://t.co/OJ67L7MlrD']\", 'pred': 0, 'true': 1}, 1918: {'tweet': '[\\'\"Light and stenographers\"\\']', 'pred': 1, 'true': 0}, 1920: {'tweet': \"['💻TELETRABAJOJOB💻\\\\n', '\\\\n', '👉 It has exploded in our country with #COVID19. \\\\n', '\\\\n', '\\\\U0001f9d1💼Most companies have been forced to implement this formula. \\\\n', '\\\\n', '🗣️A debate is opened that needs analysis and proposals.\\\\n', '\\\\n', 'Read our study on #telework \\\\n', '\\\\n', '👇👇👇\\\\n', '\\\\n', 'https://t.co/rXI30qj3za https://t.co/7Y6Za53ohw']\", 'pred': 0, 'true': 1}, 1923: {'tweet': '[\"Is that what you were clapping for every Pharisee\\'s evening ? #rebrotes #COVID19 #StateOfAlarm https://t.co/g2DQUu6Shp\"]', 'pred': 1, 'true': 0}, 1932: {'tweet': '[\\'SHEARERS from Uruguay 🇺🇾 land in Spain🇪🇸 to shear several million sheep and against the clock. For the #COVIDー19 they arrive with 2 months d delay.\\\\n\\', \\'\\\\n\\', \\'FULL VIDEO @elcampoCMM @CMM_es \\\\n\\', \\'▶️ https://t.co/8TbQskooKl\\\\n\\', \\'\\\\n\\', \\'🐷🐖👉US steps up exports at \"knockdown prices\" https://t.co/h9cl4Gk743\\']', 'pred': 1, 'true': 0}, 1953: {'tweet': \"['Juan Cotino, victim of the coronavirus, dies at the age of 70 - Periodista Digital https://t.co/hM8IywwNOU']\", 'pred': 1, 'true': 0}, 1956: {'tweet': '[\"🆕️ @TurismoEspGob has created the \\'Responsible Tourism\\' badge to recognize establishments that follow the guidelines and recommendations of the COVID-19 contagion reduction guidelines. It is free and can be downloaded here: https://t.co/Z0z4guHrKQ https://t.co/1bhkJdQeoU\"]', 'pred': 1, 'true': 0}, 1970: {'tweet': '[\\'Adriana Lastra calls García Egea a \"cockatoo\" for interrupting her during her intervention in the #Congreso #estadodealarma https://t.co/0haHfJ4owi\\']', 'pred': 1, 'true': 0}, 1992: {'tweet': \"['In bed friends, no, neither corona virus nor flu, sugar very high.']\", 'pred': 1, 'true': 0}, 1997: {'tweet': \"['Months locked up. You can travel to other provinces, go to restaurants, bars, cinemas, parks, see family and colleagues at home... only in exchange that if there are people nearby, you cover your nose and mouth with A FUCKING MASK. Is it so hard, is it so much to ask? #HateToHumanity']\", 'pred': 1, 'true': 0}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61-27cIztG6S",
        "outputId": "846db65c-5675-48c2-ee33-5ebb7b6fe950"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = y_val\n",
        "y_pred = eng_pred\n",
        "\n",
        "print(confusion_matrix(y_true, eng_pred, labels=[1,0]))\n",
        "print(classification_report(y_true, eng_pred, target_names=[\"non-prof\", \"prof\"]))\n",
        "print('f1-score:', f1_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 441   36]\n",
            " [ 121 1402]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    non-prof       0.97      0.92      0.95      1523\n",
            "        prof       0.78      0.92      0.85       477\n",
            "\n",
            "    accuracy                           0.92      2000\n",
            "   macro avg       0.88      0.92      0.90      2000\n",
            "weighted avg       0.93      0.92      0.92      2000\n",
            "\n",
            "f1-score: 0.848893166506256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT85QDphj5sM"
      },
      "source": [
        "#Combine Models for Classification Task\n",
        "We combine use the Spanish model to classify the tweets, then the English model to classify tweets translated into English. \n",
        "We add the output, and classify as 1 (Prof) if sum of outputs is greater than ..... else, class as 0 (non-Prof)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmB1v6jWJ0Pj",
        "outputId": "b5b477eb-3a5a-41de-8380-ed358d08b2cc"
      },
      "source": [
        "#Checking amount of tweets that got miscalssified in both models: Analysis first\n",
        "missed_idx_eng = np.array(list(eng_pred_dic.keys()))\n",
        "missed_idx_es = np.array(list(es_pred_dic.keys()))\n",
        "\n",
        "print(type(list(eng_pred_dic.items())))\n",
        "common_idx_missed = np.intersect1d(missed_idx_eng, missed_idx_es)\n",
        "count_overlap = len(common_idx_missed)\n",
        "\n",
        "\n",
        "print('Common values between preds in both languages:')\n",
        "print(common_idx_missed)\n",
        "\n",
        "print('Overlap count:', count_overlap)\n",
        "\n",
        "unique_incorrect = (len(missed_idx_eng) - 67)\n",
        "print(unique_incorrect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "Common values between preds in both languages:\n",
            "[   6   41   53   57  143  174  181  243  262  282  288  328  352  378\n",
            "  463  499  551  620  651  782  790  819  831  838  900  932  989 1003\n",
            " 1006 1083 1153 1159 1197 1226 1271 1277 1297 1300 1406 1439 1447 1478\n",
            " 1487 1508 1512 1570 1595 1629 1642 1669 1680 1700 1764 1791 1809 1810\n",
            " 1824 1826 1843 1856 1879 1910 1918 1921 1932 1997]\n",
            "Overlap count: 66\n",
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh4pXJvJ0DDA"
      },
      "source": [
        "##Import models for Majority Vote Method\n",
        "We import two models in Spanish and one model in English, we will compare the predicted outputs of the three models and pick the majority vote \"class\" as our final pred"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7crJ4AmtQC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a9e76d-fadb-450c-cba9-051efa246cc4"
      },
      "source": [
        "#define variable for model creation\n",
        "es_pt_model = 'bert-base-multilingual-uncased'\n",
        "eng_pt_model = 'bert-base-uncased'\n",
        "num_labels = 1\n",
        "loss = 'binary_crossentropy'\n",
        "max_len = 70\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "#Compile models\n",
        "eng_model = create_eng_transformer_model(eng_pt_model, num_labels, max_len)\n",
        "eng_model2 = create_eng_transformer_model(eng_pt_model, num_labels, max_len)\n",
        "#es_model1 = create_transformer_model(es_pt_model, num_labels, max_len)\n",
        "es_model2 = create_transformer_model(es_pt_model, num_labels, max_len)\n",
        "\n",
        "eng_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "eng_model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "#es_model1.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "es_model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "#we load both models (if needed)\n",
        "#es_model1.load_weights('/content/drive/MyDrive/Colab/saved_model/mBert_profner.h5')\n",
        "es_model2.load_weights('/content/drive/MyDrive/Colab/saved_model/mBert_profner1.hdf5')\n",
        "eng_model.load_weights('/content/drive/MyDrive/Colab/saved_model/engBert_profner.hdf5')\n",
        "eng_model2.load_weights('/content/drive/MyDrive/Colab/saved_model/engBert_profner2.hdf5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 109483009   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 109,483,011\n",
            "Trainable params: 109,483,011\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 109483009   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 109,483,011\n",
            "Trainable params: 109,483,011\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 167357185   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 167,357,187\n",
            "Trainable params: 167,357,187\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp-sD8Ow2rUw"
      },
      "source": [
        "#Import data for English and Spanish tweets\n",
        "eng_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val-eng.csv')\n",
        "es_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N-kEjB92zjl"
      },
      "source": [
        "eng_val = eng_val_data.tweet\n",
        "es_val = es_val_data.tweet\n",
        "\n",
        "labels = eng_val_data.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfn1hVXB3xpx"
      },
      "source": [
        "##Preprocess val/test (if not already)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeTBZYTN0COR",
        "outputId": "9f26c93a-3fd9-4299-de6c-9d8ea4f86760"
      },
      "source": [
        "eng_tokenizer = 'bert-base-uncased'\n",
        "es_tokenizer = 'bert-base-multilingual-uncased'\n",
        "\n",
        "es_val_input, es_val_masks = preprocess_bert(es_val, es_tokenizer)\n",
        "eng_val_input, eng_val_masks = preprocess_bert(eng_val, eng_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7RfoPo235tE"
      },
      "source": [
        "##Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSVPmMk-2rT8",
        "outputId": "735161b0-98db-4ae2-f4a8-43eb7ec3d737"
      },
      "source": [
        "#es_model1_pred = assign_class(es_model1.predict([es_val_input, es_val_masks]))\n",
        "es_model2_pred = assign_class(es_model2.predict([es_val_input, es_val_masks]))\n",
        "eng_model_pred = assign_class(eng_model.predict([eng_val_input, eng_val_masks]))\n",
        "eng_model_pred2 = assign_class(eng_model2.predict([eng_val_input, eng_val_masks]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn4IM5TFvwLS",
        "outputId": "ac610dc9-df77-411c-f3b6-803a317acf87"
      },
      "source": [
        "es_model2_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0263709 ],\n",
              "       [0.05390123],\n",
              "       [0.00559049],\n",
              "       ...,\n",
              "       [0.01828228],\n",
              "       [0.04691067],\n",
              "       [0.00566016]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-42nPZx8Op9"
      },
      "source": [
        "##Function to determine majority"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTle3TOx74ad"
      },
      "source": [
        "def majority_vote(pred1, pred2, pred3):\n",
        "  pred =[]\n",
        "  for i in range(len(pred1)):\n",
        "    sum = pred1[i] + pred2[i] + pred3[i]\n",
        "    if(sum >= 2):\n",
        "      pred.append(1)\n",
        "    else:\n",
        "      pred.append(0)\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmXNfT1D8FPq"
      },
      "source": [
        "##Confusion Matrix and Classification Report for Majority Vote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gof0QaF8EDq",
        "outputId": "e1bbff62-c838-4255-d699-d0531b4fd5ad"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_true = labels\n",
        "y_pred = majority_vote(es_model1_pred, eng_model_pred, eng_model_pred2)\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred, labels=[1,0]))\n",
        "print(classification_report(y_true, y_pred, target_names=[\"non-prof\", \"prof\"]))\n",
        "print('f1-score:', f1_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 410   67]\n",
            " [  46 1477]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    non-prof       0.96      0.97      0.96      1523\n",
            "        prof       0.90      0.86      0.88       477\n",
            "\n",
            "    accuracy                           0.94      2000\n",
            "   macro avg       0.93      0.91      0.92      2000\n",
            "weighted avg       0.94      0.94      0.94      2000\n",
            "\n",
            "f1-score: 0.8788853161843516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np8p6p_0pdip"
      },
      "source": [
        "#Combine models for Classification Task : Maximum Probability Try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POpVA8XY9Zdk"
      },
      "source": [
        "##Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bvrjw0i9KsJ"
      },
      "source": [
        "#Import data in English and in Spanish\n",
        "#Import data for English and Spanish tweets\n",
        "eng_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val-eng.csv')\n",
        "es_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val.csv')\n",
        "\n",
        "#use dataframe to assign appropriate values to variables\n",
        "eng_val = eng_val_data.tweet\n",
        "es_val = es_val_data.tweet\n",
        "\n",
        "labels = eng_val_data.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-KJwSthg7YG"
      },
      "source": [
        "#Import data in English and in Spanish\n",
        "#Import data for English and Spanish tweets\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/test.csv')\n",
        "\n",
        "#use dataframe to assign appropriate values to variables\n",
        "test = test_data.tweet\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGuTGVbThN06",
        "outputId": "d2b1be7d-c33a-4ba4-e4a3-55554d0c044f"
      },
      "source": [
        "len(test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U-AVrWf9dfE"
      },
      "source": [
        "##Preprocess data using bert using bert-base and using mbert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPPXixKH9UqQ",
        "outputId": "6ca8654d-4195-4759-fd35-743b376b34e0"
      },
      "source": [
        "eng_tokenizer = 'bert-base-uncased'\n",
        "es_tokenizer = 'bert-base-multilingual-uncased'\n",
        "max_len = 80\n",
        "\n",
        "es_val_input, es_val_masks = preprocess_bert(es_val, es_tokenizer, max_len)\n",
        "eng_val_input, eng_val_masks = preprocess_bert(eng_val, eng_tokenizer, max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "4d809e439e3f4fd0ab69bb6da5df1ddc",
            "b1cdca84f18d48f1a47aafdddc79017f",
            "85fa47a0ce4a492d9a0540d7d7dd3d86",
            "4f4c8083a0cb43f094a5e64e62aa08f0",
            "b07afb237b764ab29fe990c5d20fa4b6",
            "4412aa196f254502b9ae36dff6058426",
            "f49a24adf75044f4bd13d1c4c50b5902",
            "08f02ca8eb6e46559782fd96fd59eb30"
          ]
        },
        "id": "czlD9ZnDhRy9",
        "outputId": "d210fe57-43dc-4cd6-aa40-22099c0bee45"
      },
      "source": [
        "es_tokenizer = 'bert-base-multilingual-uncased'\n",
        "max_len = 80\n",
        "\n",
        "es_test_input, es_test_masks = preprocess_bert(test, es_tokenizer, max_len)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d809e439e3f4fd0ab69bb6da5df1ddc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1pfesfJ96l9"
      },
      "source": [
        "##Compile and load weights of es and eng model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b2dbd60736734f91ba6980877c114799",
            "4074726b0d2746b5b881a7306d38a560",
            "d7056be218704b728e866b4a21c1f4b7",
            "bf0bb9a6fa874f449e3eaea02dff5d37",
            "f10e923b924a48c2b8037b29ae45ab92",
            "42913cd3e3a24a41b075e8e6d409a2af",
            "62fc0b7264a445b2853a0b750e5d52dc",
            "38d417c033404de28ee1c9fe46c8b97a",
            "d0fc4f458ea74c1c85a42591f7707b33",
            "727fb218ca7d41ee970db1a0baf9c684",
            "a78640614a9f4678ab0e6eeba73650fa",
            "b7dbfbd1bfa946288718b07b952daf47",
            "6c0dc70eee8648db8a5e442647e64e58",
            "f0586ecabf744797b8543769de5aebec",
            "6b19b78dda6c4b07b4db5ea1d5e1e9a0",
            "19c7314654144429a30efd3adf646c70"
          ]
        },
        "id": "HUJsleqZzlfQ",
        "outputId": "59d1c3a3-38c7-4853-fd98-3f74afbb6d82"
      },
      "source": [
        "#define variable for model creation\n",
        "es_pt_model = 'bert-base-multilingual-uncased'\n",
        "eng_pt_model = 'bert-base-uncased'\n",
        "num_labels = 1\n",
        "loss = 'binary_crossentropy'\n",
        "max_len = 80\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "#Compile models\n",
        "aug_model = create_transformer_model(es_pt_model, num_labels, max_len)\n",
        "es_model = create_transformer_model(es_pt_model, num_labels, max_len)\n",
        "\n",
        "aug_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "es_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "#we load both models (if needed)\n",
        "es_model.load_weights('/content/drive/MyDrive/Colab/saved_model/mBert_profner3.hdf5')\n",
        "aug_model.load_weights('/content/drive/MyDrive/Colab/saved_model/mBert_profner_aug.hdf5')\n",
        "#eng_model2.load_weights('/content/drive/MyDrive/Colab/saved_model/engBert_profner2.hdf5')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2dbd60736734f91ba6980877c114799",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0fc4f458ea74c1c85a42591f7707b33",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=999358484.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f154d42de50>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f154d42de50>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f1568856c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f1568856c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 167357185   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 167,357,187\n",
            "Trainable params: 167,357,187\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "train_input (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "train_masks (InputLayer)        [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 167357185   train_input[0][0]                \n",
            "                                                                 train_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            2           tf_bert_for_sequence_classificati\n",
            "==================================================================================================\n",
            "Total params: 167,357,187\n",
            "Trainable params: 167,357,187\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpNrva-H-Xw3"
      },
      "source": [
        "##Predict probability of each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEAiCwyv-doJ",
        "outputId": "1d0b9efd-3aa8-4851-f918-303a17820ebf"
      },
      "source": [
        "es_prob = es_model.predict([es_test_input, es_test_masks])\n",
        "aug_prob = aug_model.predict([es_test_input, es_test_masks])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPPBr_7C_yfB"
      },
      "source": [
        "print(es_prob[:10])\n",
        "print(aug_prob[:10])\n",
        "print(labels[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiX3eUq6BDT7"
      },
      "source": [
        "def pick_max_prob(es_prob, aug_prob):\n",
        "  pred = []\n",
        "  for i in range(len(es_prob)):\n",
        "    pred.append(np.max([es_prob[i], aug_prob[i]]))\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCqpc57_coZz"
      },
      "source": [
        "###Idea to try: pick positive class from weighted class and negative class from other model (if there is a disagreement)\n",
        "'''\n",
        "es_pred = assign_class(es_prob)\n",
        "weight_pred = assign_class(aug_prob)\n",
        "pred = []\n",
        "for i in range(len(es_prob)):\n",
        "  if es_pred[i] != weight_pred[i]:\n",
        "    if es_pred[i] == 0:\n",
        "      pred.append(0)\n",
        "    else:\n",
        "      pred.append(1)\n",
        "  else:\n",
        "    if es_pred[i] == 0:\n",
        "      pred.append(0)\n",
        "    else:\n",
        "      pred.append(1)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3US-wOnmBrzl"
      },
      "source": [
        "##Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCcnTGj0BvQz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "0a85f1ae-c0c0-4186-cfed-c5e0c97b6807"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#y_true = labels\n",
        "y_pred = assign_class(pick_max_prob(es_prob, aug_prob))\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred, labels=[1,0]))\n",
        "print(classification_report(y_true, y_pred, target_names=[\"non-prof\", \"prof\"]))\n",
        "print('f1-score:', f1_score(y_true, y_pred))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3e09de428103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpick_max_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"non-prof\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prof\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f1-score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQizxGbtlvAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7489103c-0d8f-49d1-efd9-93dcb42f1c85"
      },
      "source": [
        "len(y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-oOATfyoUR"
      },
      "source": [
        "#Concatenate transformers idea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVfonSDizPyA"
      },
      "source": [
        "Step one:\n",
        "  encode Spanish text to mbert, encode English text to bert-base.\n",
        "  Encodings will be used to init a transoformer model in Spanish, and the init a transformer model in English. \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivF5mUsY-lcJ",
        "outputId": "b78941b8-d5b5-460a-bf12-32f3f9e9576e"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TScabkPU-2QN"
      },
      "source": [
        "#import data in English and Spanish for training\n",
        "eng_train_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/train-eng.csv')\n",
        "es_train_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/train.csv')\n",
        "\n",
        "eng_train = eng_train_data.tweet\n",
        "es_train = es_train_data.tweet\n",
        "\n",
        "labels = eng_train_data.label\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YREbcr8h_XpM"
      },
      "source": [
        "#shuffle data for train data in english\n",
        "idx = np.random.permutation(len(eng_train))\n",
        "eng_x_train, eng_y_train = eng_train[idx],labels[idx]\n",
        "\n",
        "#shuffle train data in Spanish\n",
        "#idx = np.random.permutation(len(es_train))\n",
        "es_x_train, es_y_train = es_train[idx],labels[idx]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEiDKN4OzPW9"
      },
      "source": [
        "#Import data in English and in Spanish Validation\n",
        "#Import data for English and Spanish tweets\n",
        "eng_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val-eng.csv')\n",
        "es_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val.csv')\n",
        "\n",
        "#use dataframe to assign appropriate values to variables\n",
        "eng_val = eng_val_data.tweet\n",
        "es_val = es_val_data.tweet\n",
        "\n",
        "val_labels = eng_val_data.label"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StbQ3uVjed0V"
      },
      "source": [
        "#Import data in English and in Spanish Validation\n",
        "#Import data for English and Spanish tweets\n",
        "es_test_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/test.csv')\n",
        "\n",
        "#use dataframe to assign appropriate values to variables\n",
        "es_test = es_test_data.tweet"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFNMJy_SfGnf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "e3e461ba429a4da29c3c3789a5e58b47",
            "423892a869a54b40a23685a855388ad1",
            "0fc26d8cc0bc4a729f7c3504cd9edf7d",
            "a304a9d8fb5a439fbbe69ef56be630e3",
            "0ddec9ab3ee44afa97e53baf993c3cc3",
            "bab7d6da392644d59796bcb59e575315",
            "383557d2814d4d33a7e12121872928ef",
            "f636baaa3e0c4beb874da7289fd02ecb"
          ]
        },
        "outputId": "4a7d47ca-c196-4053-bc33-54162bd25f5c"
      },
      "source": [
        "#Encode test data into bert eng and m bert\n",
        "eng_tokenizer = 'bert-base-uncased'\n",
        "es_tokenizer = 'bert-base-multilingual-uncased'\n",
        "max_len = 80\n",
        "\n",
        "#train\n",
        "eng_test_input, eng_test_masks = preprocess_bert(es_test, eng_tokenizer, max_len)\n",
        "es_test_input, es_test_masks = preprocess_bert(es_test, es_tokenizer, max_len)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3e461ba429a4da29c3c3789a5e58b47",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ky_VvJGyuPD",
        "outputId": "24f26e2a-e0db-46b6-87cd-3895aacf4dff"
      },
      "source": [
        "eng_tokenizer = 'bert-base-uncased'\n",
        "es_tokenizer = 'bert-base-multilingual-uncased'\n",
        "max_len = 80\n",
        "\n",
        "#train\n",
        "#eng_train_input, eng_train_masks = preprocess_bert(eng_x_train, eng_tokenizer, max_len)\n",
        "#es_train_input, es_train_masks = preprocess_bert(es_x_train, es_tokenizer, max_len)\n",
        "\n",
        "#validation\n",
        "es_val_input, es_val_masks = preprocess_bert(es_val, es_tokenizer, max_len)\n",
        "eng_val_input, eng_val_masks = preprocess_bert(eng_val, eng_tokenizer, max_len)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1sttMh08sA4"
      },
      "source": [
        "#make labels into numpy arrays, and prep x_val\n",
        "x_val = [eng_val_input, eng_val_masks, es_val_input, es_val_masks]\n",
        "\n",
        "#turn labels into numpy arrays and then concatenate\n",
        "y_train = np.asarray(eng_y_train, dtype='int32')\n",
        "#es_y_train = np.asarray(es_y_train, dtype='int32')\n",
        "#y_train = np.append(eng_y_train, es_y_train)\n",
        "\n",
        "#turn labels into numpy arrays and then concatenate (the same labels in same order bc not randomized)\n",
        "y_val = np.asarray(val_labels, dtype='int32')\n",
        "#y_val = np.append(val_labels, val_labels)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICusFM0tGfOA",
        "outputId": "c74d8d35-5513-45d0-c115-28b6dfeaaa62"
      },
      "source": [
        "print(type(y_train))\n",
        "print(y_train.shape)\n",
        "print(y_train.dtype)\n",
        "print(y_train[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(6000,)\n",
            "int32\n",
            "[0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDXdnTvs0ayb"
      },
      "source": [
        "##Create model with two transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQnge_XQ0glG"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#making labels into numpy arrays for training in transformers and keras\n",
        "def create_concat_transformer_model(pre_trained_model_1, pre_trained_model_2, num_labels, max_len):\n",
        "  eng_transformer_model = TFBertForSequenceClassification.from_pretrained(pre_trained_model_1, num_labels=num_labels)\n",
        "  es_transformer_model = TFBertForSequenceClassification.from_pretrained(pre_trained_model_2, num_labels=num_labels)\n",
        "  \n",
        "  #We create keras tensors english\n",
        "  eng_input_ids = tf.keras.layers.Input(shape=(max_len,), name='eng_train_input', dtype=tf.int32)\n",
        "  eng_input_masks = tf.keras.layers.Input(shape=(max_len), name='eng_train_masks', dtype=tf.int32)\n",
        "\n",
        "  #We create keras tensors Spanish\n",
        "  es_input_ids = tf.keras.layers.Input(shape=(max_len,), name='es_train_input', dtype=tf.int32)\n",
        "  es_input_masks = tf.keras.layers.Input(shape=(max_len), name='es_train_masks', dtype=tf.int32)\n",
        "  \n",
        "  #take into account single dimension\n",
        "  eng_seq_outputs = eng_transformer_model(eng_input_ids, eng_input_masks)[0]\n",
        "  es_seq_outputs = es_transformer_model(es_input_ids, es_input_masks)[0]\n",
        "\n",
        "  #concatenate outputs sequences from transformer models\n",
        "  concat_outputs = tf.concat([eng_seq_outputs, es_seq_outputs],1)\n",
        "  outputs = tf.keras.layers.Dense(num_labels, activation='sigmoid')(concat_outputs)\n",
        "  \n",
        "  model = tf.keras.models.Model(inputs=[eng_input_ids,eng_input_masks, es_input_ids, es_input_masks], outputs=outputs)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854,
          "referenced_widgets": [
            "d283c9a299b5448da5bbcabbd99846b1",
            "9044a38dae054ae4a8a7eb380709d985",
            "2514ec8efbd347039dbf7b554bbc9353",
            "7762d4e3fece4f519850e78860641088",
            "73e8bfbdffa640c897f252d4cac7516b",
            "57029b5844f142e8aec00f4f8e2a37b2",
            "c244b2f535c74de3b6e58290c5af4eee",
            "2fb27c1e6cd145cc8828a471517c1077",
            "a74a122c5226443e9c7a59b48fadf326",
            "0f9da1799d88441ab61b3e4aff54922b",
            "a44a9b5ce725468aa5500cfc610083f5",
            "b95916ff8e8e48aca591e125a3919d12",
            "3f0d21e5443641bfbce6f2d641bebaf1",
            "7e8dad18d49145929ecb693a63c96d5c",
            "e78f25700eb048f4b2e8b3d6e2a995b8",
            "100eeb72c0ce42eeaa7323bd3eeb410e"
          ]
        },
        "id": "ofbDXmmD6xqF",
        "outputId": "ef398566-ba89-49ad-dec4-b5144bf73270"
      },
      "source": [
        "#call create model function and compile\n",
        "eng_trans_model = 'bert-base-uncased'\n",
        "es_trans_model = 'bert-base-multilingual-uncased'\n",
        "num_labels = 1\n",
        "loss = 'binary_crossentropy'\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "\n",
        "concat_model = create_concat_transformer_model(eng_trans_model, es_trans_model, num_labels, 80)\n",
        "concat_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d283c9a299b5448da5bbcabbd99846b1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a74a122c5226443e9c7a59b48fadf326",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "eng_train_input (InputLayer)    [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "eng_train_masks (InputLayer)    [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "es_train_input (InputLayer)     [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "es_train_masks (InputLayer)     [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 109483009   eng_train_input[0][0]            \n",
            "                                                                 eng_train_masks[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_sequence_classifica TFSequenceClassifier 167357185   es_train_input[0][0]             \n",
            "                                                                 es_train_masks[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat (TFOpLambda)          (None, 2)            0           tf_bert_for_sequence_classificati\n",
            "                                                                 tf_bert_for_sequence_classificati\n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            3           tf.concat[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 276,840,197\n",
            "Trainable params: 276,840,197\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbq9_VDn64ct",
        "outputId": "b4349ad6-c789-4572-bf0b-574370acfc1b"
      },
      "source": [
        "#fit model to input data\n",
        "class_weights = {0:1.,\n",
        "                 1:30.}\n",
        "with tf.device('/device:GPU:0'):\n",
        "  concat_model.fit([eng_train_input, eng_train_masks, es_train_input, es_train_masks], y_train, validation_data=(x_val, y_val), epochs=4, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.8259WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "188/188 [==============================] - 155s 717ms/step - loss: 0.4250 - accuracy: 0.8262 - val_loss: 0.1693 - val_accuracy: 0.9450\n",
            "Epoch 2/4\n",
            "188/188 [==============================] - 133s 706ms/step - loss: 0.1299 - accuracy: 0.9578 - val_loss: 0.1554 - val_accuracy: 0.9520\n",
            "Epoch 3/4\n",
            "188/188 [==============================] - 133s 706ms/step - loss: 0.0602 - accuracy: 0.9823 - val_loss: 0.1733 - val_accuracy: 0.9410\n",
            "Epoch 4/4\n",
            "188/188 [==============================] - 133s 706ms/step - loss: 0.0339 - accuracy: 0.9900 - val_loss: 0.1734 - val_accuracy: 0.9545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onniG3m07hsY"
      },
      "source": [
        "#save model\n",
        "#concat_model.save_weights('/content/drive/MyDrive/Colab/saved_model/concat_model_weight2.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_qVXzqd8LFZ"
      },
      "source": [
        "##Predict and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YR7ozMkai6B"
      },
      "source": [
        "concat_model.load_weights('/content/drive/MyDrive/Colab/saved_model/concat_model.hdf5')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbKl6Dot8hZ5"
      },
      "source": [
        "pred = concat_model.predict(x_val)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJhSnUva8UXN",
        "outputId": "ec5fb056-2922-4f9b-916d-f6bf09a5937b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_true = y_val\n",
        "y_pred = assign_class(pred)\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred, labels=[1,0]))\n",
        "print(classification_report(y_true, y_pred, target_names=[\"non-prof\", \"prof\"]))\n",
        "print('f1-score:', f1_score(y_true, y_pred))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 433   44]\n",
            " [  50 1473]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    non-prof       0.97      0.97      0.97      1523\n",
            "        prof       0.90      0.91      0.90       477\n",
            "\n",
            "    accuracy                           0.95      2000\n",
            "   macro avg       0.93      0.94      0.94      2000\n",
            "weighted avg       0.95      0.95      0.95      2000\n",
            "\n",
            "f1-score: 0.9020833333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bzFgTcCY5iz"
      },
      "source": [
        "# Concatenate with extra transformer layer\n",
        "We concatenate both transformer output, as above, but we feed it to a keras transformer layer and then to the dense layer. \n",
        "\n",
        "Hypothesis is that it will be the best performing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMqpxqAArwgU",
        "outputId": "74eda990-d409-4f8a-87ea-4f9066e3ad02"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPFWCbaYt1yy"
      },
      "source": [
        "##Import, preprocess and tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDoNb7Hvr1SA"
      },
      "source": [
        "#import data in English and Spanish for training\n",
        "eng_train_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/train-eng.csv')\n",
        "es_train_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/train.csv')\n",
        "\n",
        "eng_train = eng_train_data.tweet\n",
        "es_train = es_train_data.tweet\n",
        "\n",
        "labels = eng_train_data.label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8RDZM99tj0h"
      },
      "source": [
        "#shuffle data for train data in english\n",
        "idx = np.random.permutation(len(eng_train))\n",
        "eng_x_train, eng_y_train = eng_train[idx],labels[idx]\n",
        "\n",
        "#shuffle train data in Spanish\n",
        "#idx = np.random.permutation(len(es_train))\n",
        "es_x_train, es_y_train = es_train[idx],labels[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F-iU0rRtpzk"
      },
      "source": [
        "#Import data in English and in Spanish Validation\n",
        "#Import data for English and Spanish tweets\n",
        "eng_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val-eng.csv')\n",
        "es_val_data = pd.read_csv('/content/drive/MyDrive/Datasets/profner/val.csv')\n",
        "\n",
        "#use dataframe to assign appropriate values to variables\n",
        "eng_val = eng_val_data.tweet\n",
        "es_val = es_val_data.tweet\n",
        "\n",
        "val_labels = eng_val_data.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193,
          "referenced_widgets": [
            "c7b864fbb7e44ba288854c7358d94b1e",
            "993108f6cb9040b7bc2e193170fec178",
            "0c49821795e44aeb8bc75e940b6f3536",
            "3f3bf269c691472688a660e19837e46f",
            "2af5eb70fa5e42b29568b580b2f40621",
            "796422cd6ef84c8fae34b032eb21d7f7",
            "725c403555944437b58eae4b1a22e511",
            "c9f81e2b6bb74407914f92b834c50761"
          ]
        },
        "id": "8IvywaEdtq3y",
        "outputId": "5240e8a2-4add-4eed-9516-19e5e3123e87"
      },
      "source": [
        "eng_tokenizer = 'bert-base-uncased'\n",
        "es_tokenizer = 'bert-base-multilingual-uncased'\n",
        "max_len = 80\n",
        "\n",
        "#train\n",
        "eng_train_input, eng_train_masks = preprocess_bert(eng_x_train, eng_tokenizer, max_len)\n",
        "es_train_input, es_train_masks = preprocess_bert(es_x_train, es_tokenizer, max_len)\n",
        "\n",
        "#validation\n",
        "es_val_input, es_val_masks = preprocess_bert(es_val, es_tokenizer, max_len)\n",
        "eng_val_input, eng_val_masks = preprocess_bert(eng_val, eng_tokenizer, max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7b864fbb7e44ba288854c7358d94b1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h_3M7YLtu4B"
      },
      "source": [
        "#make labels into numpy arrays, and prep x_val\n",
        "x_val = [eng_val_input, eng_val_masks, es_val_input, es_val_masks]\n",
        "\n",
        "#turn labels into numpy arrays and then concatenate\n",
        "y_train = np.asarray(eng_y_train, dtype='int32')\n",
        "\n",
        "#turn labels into numpy arrays and then concatenate (the same labels in same order bc not randomized)\n",
        "y_val = np.asarray(val_labels, dtype='int32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j1c2r8Gt0Gw"
      },
      "source": [
        "##Create model with keras transformer layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTT6aUw0WKcw",
        "outputId": "e1e45a85-426b-4d7e-a45f-5c87f2ae9147"
      },
      "source": [
        "!pip install keras-transformer\n",
        "import keras_transformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-transformer\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-transformer) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-transformer) (2.4.3)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-transformer) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-transformer) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-transformer) (2.10.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-transformer) (1.15.0)\n",
            "Building wheels for collected packages: keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp37-none-any.whl size=12942 sha256=a6133288ce4ba1efee17e3e9bc89212619b4644ed820457beb8dc3a13ec49e1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp37-none-any.whl size=7554 sha256=358d71c600ea569a49347b294fd77315a06914d7721c72b9b4964a2ad5d7e877\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp37-none-any.whl size=15611 sha256=e10eb2ff5573a999becb5b312ab8d3013d778a2d495414abdf5bc045118fbdf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp37-none-any.whl size=5269 sha256=717821a839c54ad7f5c49b6ed0a06535d9b2540084aa95ee488f6eee6f03d6cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp37-none-any.whl size=5623 sha256=113532abe65d1f33b3ee0c1b4041086c8255f5e585c93a438d902f695de7c241\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp37-none-any.whl size=4558 sha256=e14576b4c398ae83fb57d956de1db4f16ea715f0e2e4a21f30f67a5095b9870b\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp37-none-any.whl size=17278 sha256=dcf7b4d3162f20c1d80939b0a6ac3a8f416e9ce8c3b01802f5560c19eb2ea7f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer\n",
            "Successfully installed keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrkfushfGZzs"
      },
      "source": [
        "#transformer_block code from keras\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAh6to8WP44K"
      },
      "source": [
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4-99jBguBxJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras_transformer import get_model\n",
        "\n",
        "#making labels into numpy arrays for training in transformers and keras\n",
        "def create_transformer_model(pre_trained_model_1, pre_trained_model_2, num_labels, max_len):\n",
        "  #bert models\n",
        "  eng_transformer_model = TFBertForSequenceClassification.from_pretrained(pre_trained_model_1, num_labels=num_labels)\n",
        "  es_transformer_model = TFBertForSequenceClassification.from_pretrained(pre_trained_model_2, num_labels=num_labels)\n",
        "  \n",
        "  print('bert done')\n",
        "  #transoformer_block using keras implementation\n",
        "  transoformer_block = TransformerBlock(768 * 2, num_heads=2, ff_dim=256)\n",
        "  #embedding tokens and positioning\n",
        "  embedding_layer = TokenAndPositionEmbedding(max_len, )\n",
        "  print('transformer done')\n",
        "  #We create keras tensors english\n",
        "  eng_input_ids = tf.keras.layers.Input(shape=(max_len,), name='eng_train_input', dtype=tf.int32)\n",
        "  eng_input_masks = tf.keras.layers.Input(shape=(max_len), name='eng_train_masks', dtype=tf.int32)\n",
        "  print('input english done')\n",
        "  #We create keras tensors Spanish\n",
        "  es_input_ids = tf.keras.layers.Input(shape=(max_len,), name='es_train_input', dtype=tf.int32)\n",
        "  es_input_masks = tf.keras.layers.Input(shape=(max_len), name='es_train_masks', dtype=tf.int32)\n",
        "  print('input español done')\n",
        "\n",
        "  #take into account single dimension\n",
        "  eng_seq_outputs = eng_transformer_model(eng_input_ids, eng_input_masks)[0]\n",
        "  es_seq_outputs = es_transformer_model(es_input_ids, es_input_masks)[0]\n",
        "  print('outputs done')\n",
        "\n",
        "  #concatenate outputs sequences from transformer models\n",
        "  concat_outputs = tf.concat([eng_seq_outputs, es_seq_outputs],1)\n",
        "  print('concat_done')\n",
        "  trans_layer = transoformer_block(concat_outputs)\n",
        "  print('transfomer done')\n",
        "  trans_layer = tf.keras.layers.GlobalAveragePooling1D()(trans_layer)\n",
        "  print('avg pooling')\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(trans_layer)\n",
        "  print('outputs')\n",
        "  \n",
        "  model = tf.keras.models.Model(inputs=[eng_input_ids,eng_input_masks, es_input_ids, es_input_masks], outputs=outputs)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oceTtr3duG1r",
        "outputId": "582a1eb9-6979-4cfc-b2e4-2d6c6436ec31"
      },
      "source": [
        "#call create model function and compile\n",
        "eng_trans_model = 'bert-base-uncased'\n",
        "es_trans_model = 'bert-base-multilingual-uncased'\n",
        "num_labels = 1\n",
        "loss = 'binary_crossentropy'\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "\n",
        "concat_model = create_transformer_model(eng_trans_model, es_trans_model, num_labels, 80)\n",
        "concat_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "bert done\n",
            "transformer done\n",
            "input english done\n",
            "input español done\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "outputs done\n",
            "concat_done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "StagingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-120e4936adb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mconcat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_trans_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes_trans_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mconcat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-890ecf641ea0>\u001b[0m in \u001b[0;36mcreate_transformer_model\u001b[0;34m(pre_trained_model_1, pre_trained_model_2, num_labels, max_len)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mconcat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meng_seq_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes_seq_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'concat_done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mtrans_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransoformer_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transfomer done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mtrans_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    <ipython-input-88-4ae82adb2139>:15 call  *\n        attn_output = self.att(inputs, inputs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/multi_head_attention.py:474 call\n        query, key, value, attention_mask, training)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/multi_head_attention.py:438 _compute_attention\n        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/multi_head_attention.py:401 _masked_softmax\n        return self._softmax(attention_scores, attention_mask)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/advanced_activations.py:334 call\n        return K.softmax(inputs, axis=self.axis[0])\n\n    IndexError: tuple index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35DO7ugCuKKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7027c9cb-a1fa-4a44-8bb2-1402a0f4f8cd"
      },
      "source": [
        "#fit model to input data\n",
        "with tf.device('/device:GPU:0'):\n",
        "  concat_model.fit([eng_train_input, eng_train_masks, es_train_input, es_train_masks], y_train, validation_data=(x_val, y_val), epochs=7, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.7850WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "188/188 [==============================] - 164s 728ms/step - loss: 0.5325 - accuracy: 0.7853 - val_loss: 0.2151 - val_accuracy: 0.9370\n",
            "Epoch 2/7\n",
            "188/188 [==============================] - 134s 713ms/step - loss: 0.2138 - accuracy: 0.9373 - val_loss: 0.1906 - val_accuracy: 0.9430\n",
            "Epoch 3/7\n",
            "188/188 [==============================] - 134s 712ms/step - loss: 0.1244 - accuracy: 0.9688 - val_loss: 0.2083 - val_accuracy: 0.9395\n",
            "Epoch 4/7\n",
            "188/188 [==============================] - 134s 711ms/step - loss: 0.1054 - accuracy: 0.9730 - val_loss: 0.1985 - val_accuracy: 0.9410\n",
            "Epoch 5/7\n",
            "188/188 [==============================] - 134s 712ms/step - loss: 0.0806 - accuracy: 0.9801 - val_loss: 0.2301 - val_accuracy: 0.9335\n",
            "Epoch 6/7\n",
            "188/188 [==============================] - 134s 711ms/step - loss: 0.0817 - accuracy: 0.9789 - val_loss: 0.1994 - val_accuracy: 0.9425\n",
            "Epoch 7/7\n",
            "188/188 [==============================] - 134s 712ms/step - loss: 0.0546 - accuracy: 0.9868 - val_loss: 0.2491 - val_accuracy: 0.9440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlMLvQo5uOQ4"
      },
      "source": [
        "#save model\n",
        "concat_model.save_weights('/content/drive/MyDrive/Colab/saved_model/transformer_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hfiORu5uX2n"
      },
      "source": [
        "##Predict and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMOSDG9zug2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6ee2fb-dfc2-41d0-c648-b6373940f7ba"
      },
      "source": [
        "pred = concat_model.predict(x_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1CBQDVsucbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99276630-f299-4175-b1fb-bb57f960134c"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_true = y_val\n",
        "y_pred = assign_class(pred)\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred, labels=[1,0]))\n",
        "print(classification_report(y_true, y_pred, target_names=[\"non-prof\", \"prof\"]))\n",
        "print('f1-score:', f1_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 416   61]\n",
            " [  51 1472]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    non-prof       0.96      0.97      0.96      1523\n",
            "        prof       0.89      0.87      0.88       477\n",
            "\n",
            "    accuracy                           0.94      2000\n",
            "   macro avg       0.93      0.92      0.92      2000\n",
            "weighted avg       0.94      0.94      0.94      2000\n",
            "\n",
            "f1-score: 0.8813559322033897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "821zeAA6i1oM"
      },
      "source": [
        "#Prep submission results in tsv file format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QpxzWovUtZ81",
        "outputId": "99154193-9748-41d4-8b63-0bef7ccd311e"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1242399368143147012</td>\n",
              "      <td>Cuidados de tu mascota durante la epidemia del...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1242402119623286791</td>\n",
              "      <td>#ULTIMAHORA: Los contagios en España por #coro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1242402388574601216</td>\n",
              "      <td>Y para que veáis que nos quejamos de vicio, pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1242402392475340803</td>\n",
              "      <td>Amancio Ortega propuesto como Premio Princesa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1242402598642167808</td>\n",
              "      <td>⚠️ #covid19 #Almería Las tiendas de barrio en ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Unnamed: 0                                              tweet\n",
              "0  1242399368143147012  Cuidados de tu mascota durante la epidemia del...\n",
              "1  1242402119623286791  #ULTIMAHORA: Los contagios en España por #coro...\n",
              "2  1242402388574601216  Y para que veáis que nos quejamos de vicio, pa...\n",
              "3  1242402392475340803  Amancio Ortega propuesto como Premio Princesa ...\n",
              "4  1242402598642167808  ⚠️ #covid19 #Almería Las tiendas de barrio en ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW-YNx8TjPTc",
        "outputId": "25e8b944-5f4f-43ae-de21-eda34d0a6cae"
      },
      "source": [
        "test_data.rename(columns={'Unnamed: 0': 'tweet_id'}, inplace=True)\n",
        "test_data.tweet_id[:5]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1242399368143147012\n",
              "1    1242402119623286791\n",
              "2    1242402388574601216\n",
              "3    1242402392475340803\n",
              "4    1242402598642167808\n",
              "Name: tweet_id, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HKINmZ6Xjm6C",
        "outputId": "1bec3195-c82a-444f-f3e0-2f33c8f28d0b"
      },
      "source": [
        "results = pd.DataFrame({\n",
        "    'tweet_id': test_data.tweet_id,\n",
        "    'label': es_pred\n",
        "})\n",
        "results.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1242399368143147012</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1242402119623286791</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1242402388574601216</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1242402392475340803</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1242402598642167808</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  label\n",
              "0  1242399368143147012      0\n",
              "1  1242402119623286791      0\n",
              "2  1242402388574601216      1\n",
              "3  1242402392475340803      1\n",
              "4  1242402598642167808      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJwG7o_Xk3aS"
      },
      "source": [
        "results.to_csv('/content/drive/MyDrive/profner_results/test_results_weight.tsv', sep='\\t')"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}